{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm \n",
    "import itertools\n",
    "\n",
    "def load_attr_dict(file):\n",
    "    # 读取属性字典\n",
    "    with open(file, 'r') as f:\n",
    "        attr_dict = {}\n",
    "        for attr, attrval_list in json.load(f).items():\n",
    "            attrval_list = list(map(lambda x: x.split('='), attrval_list))\n",
    "            attr_dict[attr] = list(itertools.chain.from_iterable(attrval_list))\n",
    "    return attr_dict\n",
    "\n",
    "def match_attrval(title, attr, attr_dict):\n",
    "    # 在title中匹配属性值\n",
    "    attrvals = \"|\".join(attr_dict[attr])\n",
    "    ret = re.findall(attrvals, title)\n",
    "    # return \"{}{}\".format(attr, ''.join(ret))\n",
    "    return \"{}\".format(''.join(ret))  \n",
    "\n",
    "# load attribute dict\n",
    "attr_dict_file = \"./data/attr_to_attrvals.json\"\n",
    "attr_dict = load_attr_dict(attr_dict_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100000it [03:20, 499.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88679\n",
      "10412\n"
     ]
    }
   ],
   "source": [
    "# remove years and get attributes [coarse]\n",
    "# 裤门襟和闭合方式的属性有重合，所以需要额外的判断机制\n",
    "coarse_data = './data/train_coarse.txt'\n",
    "pos_coarse_data = './data/pos_coarse_attr.txt'\n",
    "neg_coarse_data = './data/neg_coarse.txt'\n",
    "neg_rets = []\n",
    "pos_rets = []\n",
    "delete_list = ['2017年','2018年','2019年','2020年','2021年','2022年']\n",
    "querys = attr_dict.keys()\n",
    "i = 0\n",
    "with open(coarse_data, 'r') as f:\n",
    "    for i, data in enumerate(tqdm(f)):\n",
    "        data = json.loads(data)\n",
    "        title = data['title']\n",
    "        data['title'] = title\n",
    "        \n",
    "        if data['match']['图文'] == 1:\n",
    "            for query in querys:\n",
    "                values = attr_dict[query]\n",
    "                if (query == '裤门襟') and ('裤' not in title):\n",
    "                    continue\n",
    "                if (query == '闭合方式') and ('裤' in title):\n",
    "                    continue\n",
    "                for value in values:\n",
    "                    if value in title:\n",
    "                        data['key_attr'][query] = value\n",
    "                        data['match'][query] = 1\n",
    "            if data['key_attr']:    \n",
    "                pos_rets.append(json.dumps(data, ensure_ascii=False)+'\\n')\n",
    "        else:\n",
    "            neg_rets.append(json.dumps(data, ensure_ascii=False)+'\\n')\n",
    "        # if i>500:\n",
    "        #     break\n",
    "        # i += 1\n",
    "        \n",
    "print(len(pos_rets))\n",
    "print(len(neg_rets))        \n",
    "with open(pos_coarse_data, 'w', encoding='utf-8') as f:\n",
    "    f.writelines(pos_rets)\n",
    "with open(neg_coarse_data, 'w', encoding='utf-8') as f:\n",
    "    f.writelines(neg_rets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50000/50000 [06:16<00:00, 132.69it/s]\n",
      "100%|██████████| 88679/88679 [10:29<00:00, 140.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "620990\n"
     ]
    }
   ],
   "source": [
    "data_list = []\n",
    "save_path = './data/all_attr_match.txt'\n",
    "\n",
    "with open('./data/attr_match.json', 'r', encoding='utf-8') as f:\n",
    "    attr_key = json.load(f)\n",
    "\n",
    "def get_dismatch_value(key, val, attr_key):\n",
    "    values = attr_key[key]\n",
    "    key_index = 0\n",
    "    for i in range(len(values)):\n",
    "        if val in values[i]:\n",
    "            key_index = i\n",
    "            break\n",
    "    new_index = np.random.randint(len(values))\n",
    "    while new_index == key_index:\n",
    "        new_index = np.random.randint(len(values))\n",
    "    sub_val = values[new_index]\n",
    "    new_sub_val = sub_val[np.random.randint(len(sub_val))]\n",
    "\n",
    "    return new_sub_val\n",
    "\n",
    "def get_all_attr(path):\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "        for line in tqdm(lines):\n",
    "            data = json.loads(line)\n",
    "            for key, value in data['key_attr'].items():\n",
    "                new_dic = {}\n",
    "                new_dic['feature'] = data['feature']\n",
    "                new_dic['attr'] = value\n",
    "                new_dic['attr_match'] = 1\n",
    "                data_list.append(json.dumps(new_dic, ensure_ascii=False)+'\\n')\n",
    "                new_dic = {}\n",
    "                new_dic['feature'] = data['feature']\n",
    "\n",
    "                new_dic['attr'] = get_dismatch_value(key, value, attr_key)\n",
    "                new_dic['attr_match'] = 0\n",
    "                data_list.append(json.dumps(new_dic, ensure_ascii=False)+'\\n')\n",
    "\n",
    "\n",
    "attr_path_1 = './data/train_fine.txt'\n",
    "attr_path_2 = './data/pos_coarse_attr.txt'\n",
    "get_all_attr(attr_path_1)\n",
    "get_all_attr(attr_path_2)\n",
    "\n",
    "print(len(data_list))\n",
    "\n",
    "with open(save_path, 'w', encoding='utf-8') as f:\n",
    "    f.writelines(data_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 88679/88679 [05:15<00:00, 281.51it/s]\n",
      "100%|██████████| 50000/50000 [02:56<00:00, 283.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "10412\n",
      "20824\n",
      "266946\n"
     ]
    }
   ],
   "source": [
    "from random import choice\n",
    "import json\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "def get_random_key(keys, ratio=0.7):\n",
    "    list_ratio = [0.3, 0.7, 0.9, 1]\n",
    "    ratio = choice(list_ratio)\n",
    "    l = int(len(keys) * ratio)\n",
    "    if l == 0:\n",
    "        l = 1\n",
    "    np.random.shuffle(keys)\n",
    "    return keys[:l]\n",
    "\n",
    "with open('./data/attr_match.json', 'r', encoding='utf-8') as f:\n",
    "    attr_key = json.load(f)\n",
    "def get_title_mask(title, key, val, attr_key):\n",
    "    values = attr_key[key]\n",
    "    key_index = 0\n",
    "    for i in range(len(values)):\n",
    "        if val in values[i]:\n",
    "            key_index = i\n",
    "            break\n",
    "    new_index = np.random.randint(len(values))\n",
    "    while new_index == key_index:\n",
    "        new_index = np.random.randint(len(values))\n",
    "    sub_val = values[new_index]\n",
    "    new_sub_val = sub_val[np.random.randint(len(sub_val))]\n",
    "\n",
    "    return title.replace(val, new_sub_val, 1)\n",
    "\n",
    "pos_match_list = []\n",
    "neg_match_list = []\n",
    "\n",
    "def get_all_match(path):\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "        new_dic = {}\n",
    "        for line in tqdm(lines):\n",
    "            data = json.loads(line)\n",
    "            pos_dic = {}\n",
    "            pos_dic['feature'] = data['feature']\n",
    "            pos_dic['title'] = data['title']\n",
    "            pos_dic['all_match'] = 1\n",
    "            pos_match_list.append(json.dumps(pos_dic, ensure_ascii=False)+'\\n')\n",
    "\n",
    "            new_dic = {}\n",
    "            new_dic['feature'] = data['feature']\n",
    "            new_dic['title'] = data['title']\n",
    "            #print(new_dic['title'])\n",
    "            keys = get_random_key([x for x in data['match'].keys() if x != '图文'])\n",
    "            for key in keys:\n",
    "                new_dic['title'] = get_title_mask(new_dic['title'], key, data['key_attr'][key], attr_key)\n",
    "            new_dic['all_match'] = 0\n",
    "            neg_match_list.append(json.dumps(new_dic, ensure_ascii=False)+'\\n')\n",
    "get_all_match('./data/pos_coarse_attr.txt')\n",
    "get_all_match('./data/train_fine.txt')\n",
    "\n",
    "print(len(pos_match_list) == len(neg_match_list))\n",
    "finetune_data = []\n",
    "with open('./data/neg_coarse.txt', 'r', encoding='utf-8') as f:\n",
    "    lines = f.readlines()\n",
    "    for line in lines:\n",
    "        data = json.loads(line)\n",
    "        new_dic = {}\n",
    "        new_dic['feature'] = data['feature']\n",
    "        new_dic['title'] = data['title']\n",
    "        new_dic['all_match'] = 0\n",
    "        finetune_data.append(json.dumps(new_dic, ensure_ascii=False)+'\\n')\n",
    "l = len(finetune_data)\n",
    "print(l)\n",
    "finetune_data = finetune_data + pos_match_list[:l]\n",
    "train_data = pos_match_list[l:] + neg_match_list\n",
    "np.random.shuffle(finetune_data)\n",
    "np.random.shuffle(train_data)\n",
    "print(len(finetune_data))\n",
    "print(len(train_data))\n",
    "with open('./data/train_all_match.txt', 'w', encoding='utf-8') as f:\n",
    "    f.writelines(train_data)\n",
    "with open('./data/finetune_all_match.txt', 'w', encoding='utf-8') as f:\n",
    "    f.writelines(finetune_data)\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ab7a9148291449fc4391f360f086843f6cf18b6c2df8af6702dacf91b204e632"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
