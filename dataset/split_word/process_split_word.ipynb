{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 主要是对运行equal_split_word.py之后的处理流程\n",
    "# 包括处理word_dict，得到vocab，生成vocab_split，生成nofeat版本的数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读入已处理的词表\n",
    "with open('../vocab/vocab_dict.json', 'r') as f:\n",
    "    vocab_dict = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读入未处理的词表\n",
    "with open('../vocab/word_dict.json', 'r') as f:\n",
    "    word_dict = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 统一大量颜色的别称\n",
    "color_list = ['兰','蓝','灰','绿','粉','红','黄','青','紫','白','黑','驼','橙','杏','咖','棕','啡','褐','银','金','橘','藏']\n",
    "word_list = list(word_dict.keys())\n",
    "\n",
    "def union_word_dict(word_dict, new_word, ori_word):\n",
    "    if new_word in word_dict:\n",
    "        word_dict[new_word] += word_dict[ori_word]\n",
    "    else:\n",
    "        word_dict[new_word] = word_dict[ori_word]\n",
    "\n",
    "for word in word_list:\n",
    "    if word_dict[word] < 50: # 注意这个值要与后面删除的阈值保持一致\n",
    "        # 两个特殊的颜色\n",
    "        rep_word = word\n",
    "        if '卡其' in rep_word:\n",
    "            union_word_dict(word_dict, '卡其', word)\n",
    "            rep_word = rep_word.replace('卡其', '')\n",
    "        if '咖啡' in rep_word:\n",
    "            union_word_dict(word_dict, '咖啡', word)\n",
    "            rep_word = rep_word.replace('咖啡', '')\n",
    "        for char in rep_word:\n",
    "            if char in color_list:\n",
    "                union_word_dict(word_dict, char, word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读入属性词表\n",
    "def get_dict(file):\n",
    "    with open(file, 'r') as f:\n",
    "        all_attr = []\n",
    "        for attr, attrval_list in json.load(f).items():\n",
    "            for x in attrval_list:\n",
    "                all_attr.append(x)\n",
    "    return all_attr\n",
    "attr_dict_file = '../../data/equal_processed_data/attr_to_attrvals.json'\n",
    "all_attr = get_dict(attr_dict_file)\n",
    "\n",
    "# 删除出现次数少的词，不删除属性词，不删除我们设置的颜色值（否则后面可能会不匹配出现bug）\n",
    "ignore_list = all_attr + color_list + ['卡其', '咖啡']\n",
    "word_list = list(word_dict.keys())\n",
    "for word in word_list:\n",
    "    if word_dict[word] < 50 and word not in ignore_list:\n",
    "        del word_dict[word]\n",
    "    if word == '/':\n",
    "        del word_dict[word]\n",
    "        \n",
    "# 去掉没有单独意义的字词\n",
    "delete_words = ['色','小','本','中','新','款','加','底','件','不']\n",
    "for word in delete_words:\n",
    "    del word_dict[word]\n",
    "    \n",
    "# 保存处理后的词表\n",
    "vocab_dict = word_dict\n",
    "with open('../vocab/vocab_dict.json', 'w') as f:\n",
    "    json.dump(vocab_dict, f, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "45000it [00:49, 904.70it/s]\n"
     ]
    }
   ],
   "source": [
    "# [fine] 根据处理后的词表对数据的分词进行筛选\n",
    "fine_file = '../../data/split_word/fine45000.txt'\n",
    "save_fine_file = '../../data/equal_split_word/fine45000.txt'\n",
    "\n",
    "vocab_list = list(vocab_dict.keys())\n",
    "color_list = ['兰','蓝','灰','绿','粉','红','黄','青','紫','白','黑','驼','橙','杏','咖','棕','啡','褐','银','金','橘','藏']\n",
    "mul_color_list = ['卡其', '咖啡']\n",
    "rets = []\n",
    "\n",
    "with open(fine_file, 'r') as f:\n",
    "    for i, line in enumerate(tqdm(f)):\n",
    "        item = json.loads(line)\n",
    "        title_split = item['title_split']\n",
    "        \n",
    "        vocab_split = []\n",
    "        for word in title_split:\n",
    "            if word in vocab_list:\n",
    "                vocab_split.append(word)\n",
    "            else: # 颜色提取\n",
    "                rep_word = word\n",
    "                for mul_color in mul_color_list:\n",
    "                    if mul_color in rep_word:\n",
    "                        vocab_split.append(mul_color)\n",
    "                        rep_word = rep_word.replace(mul_color, '')\n",
    "                for char in rep_word:\n",
    "                    if char in color_list:\n",
    "                        vocab_split.append(char)\n",
    "        item['vocab_split'] = vocab_split\n",
    "        # 看看有没有空的vocab_split\n",
    "        if not vocab_split:\n",
    "            print(item['title'])\n",
    "        # 更改保存的顺序，便于查看\n",
    "        feature = item['feature']\n",
    "        del item['feature']\n",
    "        item['feature'] = feature\n",
    "        \n",
    "        rets.append(json.dumps(item, ensure_ascii=False)+'\\n')\n",
    "        \n",
    "        # if i>500:\n",
    "        #     break\n",
    "        # i += 1\n",
    "        \n",
    "with open(save_fine_file, 'w') as f:\n",
    "    f.writelines(rets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "89588it [01:39, 900.90it/s]\n"
     ]
    }
   ],
   "source": [
    "# [coarse] 根据处理后的词表对数据的分词进行筛选\n",
    "coarse_file = '../../data/split_word/coarse89588.txt'\n",
    "save_coarse_file = '../../data/equal_split_word/coarse89588.txt'\n",
    "\n",
    "vocab_list = list(vocab_dict.keys())\n",
    "color_list = ['兰','蓝','灰','绿','粉','红','黄','青','紫','白','黑','驼','橙','杏','咖','棕','啡','褐','银','金','橘','藏']\n",
    "mul_color_list = ['卡其', '咖啡']\n",
    "rets = []\n",
    "\n",
    "with open(coarse_file, 'r') as f:\n",
    "    for i, line in enumerate(tqdm(f)):\n",
    "        item = json.loads(line)\n",
    "        title_split = item['title_split']\n",
    "        \n",
    "        vocab_split = []\n",
    "        for word in title_split:\n",
    "            if word in vocab_list:\n",
    "                vocab_split.append(word)\n",
    "            else: # 颜色提取\n",
    "                rep_word = word\n",
    "                for mul_color in mul_color_list:\n",
    "                    if mul_color in rep_word:\n",
    "                        vocab_split.append(mul_color)\n",
    "                        rep_word = rep_word.replace(mul_color, '')\n",
    "                for char in rep_word:\n",
    "                    if char in color_list:\n",
    "                        vocab_split.append(char)\n",
    "        item['vocab_split'] = vocab_split\n",
    "        # 看看有没有空的vocab_split\n",
    "        if not vocab_split:\n",
    "            print(item['title'])\n",
    "        # 更改保存的顺序，便于查看\n",
    "        feature = item['feature']\n",
    "        del item['feature']\n",
    "        item['feature'] = feature\n",
    "        \n",
    "        rets.append(json.dumps(item, ensure_ascii=False)+'\\n')\n",
    "        \n",
    "        # if i>500:\n",
    "        #     break\n",
    "        # i += 1\n",
    "        \n",
    "with open(save_coarse_file, 'w') as f:\n",
    "    f.writelines(rets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "45000it [00:12, 3573.45it/s]\n"
     ]
    }
   ],
   "source": [
    "# [fine]生成nofeat版本\n",
    "fine_file = '../../data/equal_split_word/fine45000.txt'\n",
    "save_fine_file = '../../data/equal_split_word/nofeat/fine45000.txt'\n",
    "\n",
    "rets = []\n",
    "with open(fine_file, 'r') as f:\n",
    "    for i, line in enumerate(tqdm(f)):\n",
    "        item = json.loads(line)\n",
    "        del item['feature']\n",
    "        rets.append(json.dumps(item, ensure_ascii=False)+'\\n')\n",
    "        \n",
    "with open(save_fine_file, 'w') as f:\n",
    "    f.writelines(rets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "89588it [00:25, 3528.09it/s]\n"
     ]
    }
   ],
   "source": [
    "# [coarse]生成nofeat版本\n",
    "coarse_file = '../../data/equal_split_word/coarse89588.txt'\n",
    "save_coarse_file = '../../data/equal_split_word/nofeat/coarse89588.txt'\n",
    "\n",
    "rets = []\n",
    "with open(coarse_file, 'r') as f:\n",
    "    for i, line in enumerate(tqdm(f)):\n",
    "        item = json.loads(line)\n",
    "        del item['feature']\n",
    "        rets.append(json.dumps(item, ensure_ascii=False)+'\\n')\n",
    "        \n",
    "with open(save_coarse_file, 'w') as f:\n",
    "    f.writelines(rets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5000it [00:05, 886.22it/s]\n"
     ]
    }
   ],
   "source": [
    "# [fine5000] 根据处理后的词表对数据的分词进行筛选\n",
    "fine_file = '../../data/split_word/fine5000.txt'\n",
    "save_fine_file = '../../data/equal_split_word/fine5000.txt'\n",
    "\n",
    "vocab_list = list(vocab_dict.keys())\n",
    "color_list = ['兰','蓝','灰','绿','粉','红','黄','青','紫','白','黑','驼','橙','杏','咖','棕','啡','褐','银','金','橘','藏']\n",
    "mul_color_list = ['卡其', '咖啡']\n",
    "rets = []\n",
    "\n",
    "with open(fine_file, 'r') as f:\n",
    "    for i, line in enumerate(tqdm(f)):\n",
    "        item = json.loads(line)\n",
    "        title_split = item['title_split']\n",
    "        \n",
    "        vocab_split = []\n",
    "        for word in title_split:\n",
    "            if word in vocab_list:\n",
    "                vocab_split.append(word)\n",
    "            else: # 颜色提取\n",
    "                rep_word = word\n",
    "                for mul_color in mul_color_list:\n",
    "                    if mul_color in rep_word:\n",
    "                        vocab_split.append(mul_color)\n",
    "                        rep_word = rep_word.replace(mul_color, '')\n",
    "                for char in rep_word:\n",
    "                    if char in color_list:\n",
    "                        vocab_split.append(char)\n",
    "        item['vocab_split'] = vocab_split\n",
    "        # 看看有没有空的vocab_split\n",
    "        if not vocab_split:\n",
    "            print(item['title'])\n",
    "        # 更改保存的顺序，便于查看\n",
    "        feature = item['feature']\n",
    "        del item['feature']\n",
    "        item['feature'] = feature\n",
    "        \n",
    "        rets.append(json.dumps(item, ensure_ascii=False)+'\\n')\n",
    "        \n",
    "        # if i>500:\n",
    "        #     break\n",
    "        # i += 1\n",
    "        \n",
    "with open(save_fine_file, 'w') as f:\n",
    "    f.writelines(rets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10412it [00:11, 904.20it/s]\n"
     ]
    }
   ],
   "source": [
    "# [coarse10412] 根据处理后的词表对数据的分词进行筛选\n",
    "coarse_file = '../../data/split_word/coarse10412.txt'\n",
    "save_coarse_file = '../../data/equal_split_word/coarse10412.txt'\n",
    "\n",
    "vocab_list = list(vocab_dict.keys())\n",
    "color_list = ['兰','蓝','灰','绿','粉','红','黄','青','紫','白','黑','驼','橙','杏','咖','棕','啡','褐','银','金','橘','藏']\n",
    "mul_color_list = ['卡其', '咖啡']\n",
    "rets = []\n",
    "\n",
    "with open(coarse_file, 'r') as f:\n",
    "    for i, line in enumerate(tqdm(f)):\n",
    "        item = json.loads(line)\n",
    "        title_split = item['title_split']\n",
    "        \n",
    "        vocab_split = []\n",
    "        for word in title_split:\n",
    "            if word in vocab_list:\n",
    "                vocab_split.append(word)\n",
    "            else: # 颜色提取\n",
    "                rep_word = word\n",
    "                for mul_color in mul_color_list:\n",
    "                    if mul_color in rep_word:\n",
    "                        vocab_split.append(mul_color)\n",
    "                        rep_word = rep_word.replace(mul_color, '')\n",
    "                for char in rep_word:\n",
    "                    if char in color_list:\n",
    "                        vocab_split.append(char)\n",
    "        item['vocab_split'] = vocab_split\n",
    "        # 看看有没有空的vocab_split\n",
    "        if not vocab_split:\n",
    "            print(item['title'])\n",
    "        # 更改保存的顺序，便于查看\n",
    "        feature = item['feature']\n",
    "        del item['feature']\n",
    "        item['feature'] = feature\n",
    "        \n",
    "        rets.append(json.dumps(item, ensure_ascii=False)+'\\n')\n",
    "        \n",
    "        # if i>500:\n",
    "        #     break\n",
    "        # i += 1\n",
    "        \n",
    "with open(save_coarse_file, 'w') as f:\n",
    "    f.writelines(rets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5000it [00:01, 3539.21it/s]\n"
     ]
    }
   ],
   "source": [
    "# [fine5000]生成nofeat版本\n",
    "fine_file = '../../data/equal_split_word/fine5000.txt'\n",
    "save_fine_file = '../../data/equal_split_word/nofeat/fine5000.txt'\n",
    "\n",
    "rets = []\n",
    "with open(fine_file, 'r') as f:\n",
    "    for i, line in enumerate(tqdm(f)):\n",
    "        item = json.loads(line)\n",
    "        del item['feature']\n",
    "        rets.append(json.dumps(item, ensure_ascii=False)+'\\n')\n",
    "        \n",
    "with open(save_fine_file, 'w') as f:\n",
    "    f.writelines(rets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10412it [00:02, 3619.07it/s]\n"
     ]
    }
   ],
   "source": [
    "# [coarse10412]生成nofeat版本\n",
    "coarse_file = '../../data/equal_split_word/coarse10412.txt'\n",
    "save_coarse_file = '../../data/equal_split_word/nofeat/coarse10412.txt'\n",
    "\n",
    "rets = []\n",
    "with open(coarse_file, 'r') as f:\n",
    "    for i, line in enumerate(tqdm(f)):\n",
    "        item = json.loads(line)\n",
    "        del item['feature']\n",
    "        rets.append(json.dumps(item, ensure_ascii=False)+'\\n')\n",
    "        \n",
    "with open(save_coarse_file, 'w') as f:\n",
    "    f.writelines(rets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10000it [00:11, 893.23it/s]\n"
     ]
    }
   ],
   "source": [
    "# [test] 根据处理后的词表对数据的分词进行筛选\n",
    "coarse_file = '../../data/split_word/test10000.txt'\n",
    "save_coarse_file = '../../data/equal_split_word/test10000.txt'\n",
    "\n",
    "vocab_list = list(vocab_dict.keys())\n",
    "color_list = ['兰','蓝','灰','绿','粉','红','黄','青','紫','白','黑','驼','橙','杏','咖','棕','啡','褐','银','金','橘','藏']\n",
    "mul_color_list = ['卡其', '咖啡']\n",
    "rets = []\n",
    "\n",
    "with open(coarse_file, 'r') as f:\n",
    "    for i, line in enumerate(tqdm(f)):\n",
    "        item = json.loads(line)\n",
    "        title_split = item['title_split']\n",
    "        \n",
    "        vocab_split = []\n",
    "        for word in title_split:\n",
    "            if word in vocab_list:\n",
    "                vocab_split.append(word)\n",
    "            else: # 颜色提取\n",
    "                rep_word = word\n",
    "                for mul_color in mul_color_list:\n",
    "                    if mul_color in rep_word:\n",
    "                        vocab_split.append(mul_color)\n",
    "                        rep_word = rep_word.replace(mul_color, '')\n",
    "                for char in rep_word:\n",
    "                    if char in color_list:\n",
    "                        vocab_split.append(char)\n",
    "        item['vocab_split'] = vocab_split\n",
    "        # 看看有没有空的vocab_split\n",
    "        if not vocab_split:\n",
    "            print(item['title'])\n",
    "        # 更改保存的顺序，便于查看\n",
    "        feature = item['feature']\n",
    "        del item['feature']\n",
    "        item['feature'] = feature\n",
    "        \n",
    "        rets.append(json.dumps(item, ensure_ascii=False)+'\\n')\n",
    "        \n",
    "        # if i>500:\n",
    "        #     break\n",
    "        # i += 1\n",
    "        \n",
    "with open(save_coarse_file, 'w') as f:\n",
    "    f.writelines(rets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10000it [00:02, 3564.92it/s]\n"
     ]
    }
   ],
   "source": [
    "# [test]生成nofeat版本\n",
    "file = '../../data/equal_split_word/test10000.txt'\n",
    "save_file = '../../data/equal_split_word/nofeat/test10000.txt'\n",
    "\n",
    "rets = []\n",
    "with open(file, 'r') as f:\n",
    "    for i, line in enumerate(tqdm(f)):\n",
    "        item = json.loads(line)\n",
    "        del item['feature']\n",
    "        rets.append(json.dumps(item, ensure_ascii=False)+'\\n')\n",
    "        \n",
    "with open(save_file, 'w') as f:\n",
    "    f.writelines(rets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a7cf278074aea429de13d88b394304564a7b018f3e18e13089daf4fad90abe0d"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
