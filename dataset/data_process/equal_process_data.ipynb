{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 这里的equal process是针对于已经分词后的文件处理的\n",
    "# 1.创建equal_dict，用来替换的字典\n",
    "# 2.使用'title_split'分段，发现equal_dict中的key属性即在title中进行替换\n",
    "# 3.'拉链'分为'裤拉链' '鞋拉链' '拉链'，'系带'分为'裤系带' '鞋系带' '系带'\n",
    "# 4.同时重新处理title为统一大写，只需要将替换后的'title_split'合并即可\n",
    "# 5.重新生成新的合并后的processed_word_dict，并重新生成'vocab_split'分段\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm \n",
    "import json\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "equal_dict = {'半高领': '高领',\n",
    " '立领': '高领',\n",
    " '可脱卸帽': '连帽',\n",
    " '衬衫领': '翻领',\n",
    " 'POLO领': '翻领',\n",
    " '方领': '翻领',\n",
    " '娃娃领': '翻领',\n",
    " '荷叶领': '翻领',\n",
    " '五分袖': '短袖',\n",
    " '九分袖': '长袖',\n",
    " '超短款': '短款',\n",
    " '常规款': '短款',\n",
    " '超长款': '长款',\n",
    " '标准型': '修身型',\n",
    " '超短裙': '短裙',\n",
    " '中长裙': '中裙', \n",
    " 'O型裤': '哈伦裤',\n",
    " '灯笼裤': '哈伦裤',\n",
    " '锥形裤': '哈伦裤',\n",
    " '铅笔裤': '直筒裤',\n",
    " '小脚裤': '直筒裤',\n",
    " '微喇裤': '喇叭裤',\n",
    " '九分裤': '长裤',\n",
    " '套筒': '一脚蹬',\n",
    " '套脚': '一脚蹬',\n",
    " '中帮': '高帮'}\n",
    "# 替换的工作要额外做一个，中长款替换为中款！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载原始的属性字典\n",
    "def load_attr_dict(file):\n",
    "    # 读取属性字典\n",
    "    with open(file, 'r') as f:\n",
    "        attr_dict = {}\n",
    "        for attr, attrval_list in json.load(f).items():\n",
    "            attrval_list = list(map(lambda x: x.split('='), attrval_list))\n",
    "            attr_dict[attr] = list(itertools.chain.from_iterable(attrval_list))\n",
    "    return attr_dict\n",
    "\n",
    "# load attribute dict\n",
    "attr_dict_file = \"../../data/original_data/attr_to_attrvals.json\"\n",
    "attr_dict = load_attr_dict(attr_dict_file)\n",
    "\n",
    "# 相等替换\n",
    "for query, attrs in attr_dict.items():\n",
    "    attrs = attrs.copy()\n",
    "    for i, attr in enumerate(attrs):\n",
    "        if attr in equal_dict:\n",
    "            attr_dict[query].remove(attr)\n",
    "            \n",
    "# 特殊的几个属性替换\n",
    "for query, attrs in attr_dict.items():\n",
    "    attrs = attrs.copy()\n",
    "    for i, attr in enumerate(attrs):\n",
    "        if query=='衣长' and attr=='中长款':\n",
    "            attr_dict[query][i] = '中款'\n",
    "        if query=='裤门襟' and attr=='拉链':\n",
    "            attr_dict[query][i] = '拉链裤'\n",
    "        if query=='裤门襟' and attr=='系带':\n",
    "            attr_dict[query][i] = '系带裤'\n",
    "        if query=='裤门襟' and attr=='松紧':\n",
    "            attr_dict[query][i] = '松紧裤'\n",
    "        if query=='闭合方式' and attr=='拉链':\n",
    "            attr_dict[query][i] = '拉链鞋'\n",
    "        if query=='闭合方式' and attr=='系带':\n",
    "            attr_dict[query][i] = '系带鞋'\n",
    "\n",
    "# 保存新的属性字典\n",
    "attr_save_file = '../../data/equal_processed_data/attr_to_attrvals.json'\n",
    "with open(attr_save_file, 'w') as f:\n",
    "    json.dump(attr_dict, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载新的属性字典\n",
    "attr_file = '../../data/equal_processed_data/attr_to_attrvals.json'\n",
    "with open(attr_file, 'r') as f:\n",
    "    attr_dict = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "50000it [00:54, 914.90it/s]\n"
     ]
    }
   ],
   "source": [
    "# [fine] 移除年份，统一大写，替换相等属性，替换特殊属性\n",
    "fine_file = '../../data/original_data/train_fine.txt'\n",
    "new_fine_file = '../../data/equal_processed_data/fine50000.txt'\n",
    "\n",
    "rets = []\n",
    "years = ['2017年','2018年','2019年','2020年','2021年','2022年']\n",
    "\n",
    "with open(fine_file, 'r') as f:\n",
    "    for i, data in enumerate(tqdm(f)):\n",
    "        data = json.loads(data)\n",
    "        title = data['title']\n",
    "        key_attr = data['key_attr']\n",
    "        # 删除年份\n",
    "        for year in years:\n",
    "            title = title.replace(year, '')\n",
    "        # 统一大写\n",
    "        title = title.upper() # 字母统一为大写\n",
    "        # 属性替换\n",
    "        for query, attr in key_attr.items():\n",
    "            # 替换相同属性，fine的替换是从属性反向推回到title的替换\n",
    "            if attr in equal_dict:\n",
    "                key_attr[query] = equal_dict[attr]\n",
    "                # equal_dict的选词很讲究，大多是长词替换成短词，避免了replace可能的出错\n",
    "                # replace会替换所有满足条件的词，虽然可能都只有一次\n",
    "                title = title.replace(attr, equal_dict[attr]) \n",
    "            # 替换特殊属性\n",
    "            if query=='衣长' and attr=='中长款':\n",
    "                key_attr[query] = '中款'\n",
    "                title = title.replace(attr, '中款')\n",
    "            if query=='裤门襟' and attr=='拉链' and '无拉链' not in title:\n",
    "                key_attr[query] = '拉链裤'\n",
    "                title = title.replace(attr, '拉链裤')\n",
    "            if query=='裤门襟' and attr=='系带':\n",
    "                key_attr[query] = '系带裤'\n",
    "                title = title.replace(attr, '系带裤')\n",
    "            if query=='裤门襟' and attr=='松紧':\n",
    "                key_attr[query] = '松紧裤'\n",
    "                title = title.replace(attr, '松紧裤')\n",
    "            if query=='闭合方式' and attr=='拉链':\n",
    "                key_attr[query] = '拉链鞋'\n",
    "                title = title.replace(attr, '拉链鞋')\n",
    "            if query=='闭合方式' and attr=='系带':\n",
    "                key_attr[query] = '系带鞋'\n",
    "                title = title.replace(attr, '系带鞋')\n",
    "        # 一个高频词的特殊处理\n",
    "        if '厚度常规' in title:\n",
    "            title = title.replace('厚度常规', '常规厚度')\n",
    "        \n",
    "        data['key_attr'] = key_attr\n",
    "        data['title'] = title\n",
    "        # del data['feature']\n",
    "        \n",
    "        rets.append(json.dumps(data, ensure_ascii=False)+'\\n')\n",
    "        \n",
    "        # if i>500:\n",
    "        #     break\n",
    "        # i += 1\n",
    "          \n",
    "with open(new_fine_file, 'w') as f:\n",
    "    f.writelines(rets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'data/tmp_data/equal_processed_data/lala'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os \n",
    "save_dir = 'data/tmp_data/equal_processed_data'\n",
    "os.path.join(save_dir, 'lala')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100000it [01:51, 895.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "89588\n",
      "10412\n"
     ]
    }
   ],
   "source": [
    "# [coarse] 移除年份，统一大写，替换相等属性，替换特殊属性\n",
    "coarse_file = '../../data/original_data/train_coarse.txt'\n",
    "pos_coarse_file = '../../data/equal_processed_data/coarse89588.txt'\n",
    "neg_coarse_file = '../../data/equal_processed_data/coarse10412.txt'\n",
    "\n",
    "pos_rets = []\n",
    "neg_rets = []\n",
    "years = ['2017年','2018年','2019年','2020年','2021年','2022年']\n",
    "\n",
    "equal_list = list(equal_dict.keys())\n",
    "query_list = list(attr_dict.keys()) # 注意是新属性字典\n",
    "with open(coarse_file, 'r') as f:\n",
    "    for i, data in enumerate(tqdm(f)):\n",
    "        data = json.loads(data)\n",
    "        title = data['title']\n",
    "        key_attr = {}\n",
    "        # 删除年份\n",
    "        for year in years:\n",
    "            title = title.replace(year, '')\n",
    "        # 统一大写\n",
    "        title = title.upper() # 字母统一为大写\n",
    "        # 由于替换后的属性不存在包含的情况，用来做属性提取不易出错，所以先做属性替换\n",
    "        # 相同属性替换\n",
    "        for attr in equal_list:\n",
    "            if attr in title:\n",
    "                title = title.replace(attr, equal_dict[attr])\n",
    "        # 特殊属性替换\n",
    "        if '中长款' in title:\n",
    "            title = title.replace('中长款', '中款')\n",
    "        if '拉链' in title and '裤' in title and '无拉链' not in title:\n",
    "            title = title.replace('拉链', '拉链裤')\n",
    "        if '系带' in title and '裤' in title:\n",
    "            title = title.replace('系带', '系带裤')\n",
    "        if '松紧' in title and '裤' in title:\n",
    "            title = title.replace('松紧', '松紧裤')\n",
    "        if '拉链' in title and ('鞋' in title or '靴' in title):\n",
    "            title = title.replace('拉链', '拉链鞋')\n",
    "        if '系带' in title and ('鞋' in title or '靴' in title):\n",
    "            title = title.replace('系带', '系带鞋')\n",
    "        # 一个高频词的特殊处理\n",
    "        if '厚度常规' in title:\n",
    "            title = title.replace('厚度常规', '常规厚度')\n",
    "        # 属性提取\n",
    "        if data['match']['图文'] == 1:\n",
    "            for query in query_list:\n",
    "                attr_list = attr_dict[query]\n",
    "                for attr in attr_list:\n",
    "                    if attr in title:\n",
    "                        key_attr[query] = attr\n",
    "                        data['match'][query] = 1   \n",
    "            \n",
    "        data['key_attr'] = key_attr\n",
    "        data['title'] = title\n",
    "        # del data['feature']\n",
    "        \n",
    "        if data['match']['图文'] == 1:\n",
    "            pos_rets.append(json.dumps(data, ensure_ascii=False)+'\\n')\n",
    "        else:\n",
    "            neg_rets.append(json.dumps(data, ensure_ascii=False)+'\\n')\n",
    "        \n",
    "        # if i>500:\n",
    "        #     break\n",
    "        # i += 1\n",
    "        \n",
    "print(len(pos_rets))\n",
    "print(len(neg_rets))\n",
    "with open(pos_coarse_file, 'w') as f:\n",
    "    f.writelines(pos_rets)\n",
    "with open(neg_coarse_file, 'w') as f:\n",
    "    f.writelines(neg_rets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "50000it [00:13, 3637.48it/s]\n"
     ]
    }
   ],
   "source": [
    "# [fine]生成nofeat版本\n",
    "file = '../../data/equal_processed_data/fine50000.txt'\n",
    "save_file = '../../data/equal_processed_data/nofeat/fine50000.txt'\n",
    "\n",
    "rets = []\n",
    "with open(file, 'r') as f:\n",
    "    for i, line in enumerate(tqdm(f)):\n",
    "        item = json.loads(line)\n",
    "        del item['feature']\n",
    "        rets.append(json.dumps(item, ensure_ascii=False)+'\\n')\n",
    "        \n",
    "with open(save_file, 'w') as f:\n",
    "    f.writelines(rets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "89588it [00:24, 3615.17it/s]\n",
      "10412it [00:02, 3685.96it/s]\n"
     ]
    }
   ],
   "source": [
    "# [coarse]生成nofeat版本\n",
    "file = '../../data/equal_processed_data/coarse89588.txt'\n",
    "save_file = '../../data/equal_processed_data/nofeat/coarse89588.txt'\n",
    "\n",
    "rets = []\n",
    "with open(file, 'r') as f:\n",
    "    for i, line in enumerate(tqdm(f)):\n",
    "        item = json.loads(line)\n",
    "        del item['feature']\n",
    "        rets.append(json.dumps(item, ensure_ascii=False)+'\\n')\n",
    "        \n",
    "with open(save_file, 'w') as f:\n",
    "    f.writelines(rets)\n",
    "\n",
    "file = '../../data/equal_processed_data/coarse10412.txt'\n",
    "save_file = '../../data/equal_processed_data/nofeat/coarse10412.txt'\n",
    "\n",
    "rets = []\n",
    "with open(file, 'r') as f:\n",
    "    for i, line in enumerate(tqdm(f)):\n",
    "        item = json.loads(line)\n",
    "        del item['feature']\n",
    "        rets.append(json.dumps(item, ensure_ascii=False)+'\\n')\n",
    "        \n",
    "with open(save_file, 'w') as f:\n",
    "    f.writelines(rets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "50000it [00:54, 914.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45000\n",
      "5000\n"
     ]
    }
   ],
   "source": [
    "# [fine] 划分train val数据\n",
    "fine_path = '../../data/equal_processed_data/fine50000.txt'\n",
    "\n",
    "fine_train_path = '../../data/equal_processed_data/fine45000.txt'\n",
    "fine_val_path = '../../data/equal_processed_data/fine5000.txt'\n",
    "# fine_train_path = '../../data/equal_processed_data/nofeat/fine45000_nofeat.txt'\n",
    "# fine_val_path = '../../data/equal_processed_data/nofeat/fine5000_nofeat.txt'\n",
    "\n",
    "\n",
    "train_rets = []\n",
    "val_rets = []\n",
    "\n",
    "with open(fine_path, 'r') as f:\n",
    "    for i, data in enumerate(tqdm(f)):\n",
    "        data = json.loads(data)\n",
    "        # del data['feature']\n",
    "        \n",
    "        if len(train_rets) < 45000:      \n",
    "            train_rets.append(json.dumps(data, ensure_ascii=False)+'\\n')\n",
    "        else:\n",
    "            val_rets.append(json.dumps(data, ensure_ascii=False)+'\\n')\n",
    "        \n",
    "        # if i>500:\n",
    "        #     break\n",
    "        # i += 1\n",
    "        \n",
    "print(len(train_rets))\n",
    "print(len(val_rets))\n",
    "\n",
    "with open(fine_train_path, 'w') as f:\n",
    "    f.writelines(train_rets)\n",
    "with open(fine_val_path, 'w') as f:\n",
    "    f.writelines(val_rets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "45000it [00:12, 3609.15it/s]\n",
      "5000it [00:01, 3610.21it/s]\n"
     ]
    }
   ],
   "source": [
    "# 生成nofeat版本\n",
    "file = '../../data/equal_processed_data/fine45000.txt'\n",
    "save_file = '../../data/equal_processed_data/nofeat/fine45000.txt'\n",
    "\n",
    "rets = []\n",
    "with open(file, 'r') as f:\n",
    "    for i, line in enumerate(tqdm(f)):\n",
    "        item = json.loads(line)\n",
    "        del item['feature']\n",
    "        rets.append(json.dumps(item, ensure_ascii=False)+'\\n')\n",
    "        \n",
    "with open(save_file, 'w') as f:\n",
    "    f.writelines(rets)\n",
    "    \n",
    "file = '../../data/equal_processed_data/fine5000.txt'\n",
    "save_file = '../../data/equal_processed_data/nofeat/fine5000.txt'\n",
    "\n",
    "rets = []\n",
    "with open(file, 'r') as f:\n",
    "    for i, line in enumerate(tqdm(f)):\n",
    "        item = json.loads(line)\n",
    "        del item['feature']\n",
    "        rets.append(json.dumps(item, ensure_ascii=False)+'\\n')\n",
    "        \n",
    "with open(save_file, 'w') as f:\n",
    "    f.writelines(rets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10000it [00:11, 901.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n"
     ]
    }
   ],
   "source": [
    "# [test] 基础处理同上，唯一的区别是根据query提取key_attr\n",
    "test_file = '../../data/original_data/preliminary_testB.txt'\n",
    "test_save_file = '../../data/equal_processed_data/test10000.txt'\n",
    "\n",
    "rets = []\n",
    "\n",
    "years = ['2017年','2018年','2019年','2020年','2021年','2022年']\n",
    "equal_list = list(equal_dict.keys())\n",
    "with open(test_file, 'r') as f:\n",
    "    for i, data in enumerate(tqdm(f)):\n",
    "        data = json.loads(data)\n",
    "        title = data['title']\n",
    "        key_attr = {}\n",
    "        # 删除年份\n",
    "        for year in years:\n",
    "            title = title.replace(year, '')\n",
    "        # 统一大写\n",
    "        title = title.upper() # 字母统一为大写\n",
    "        # 由于替换后的属性不存在包含的情况，用来做属性提取不易出错，所以先做属性替换\n",
    "        # 相同属性替换\n",
    "        for attr in equal_list:\n",
    "            if attr in title:\n",
    "                title = title.replace(attr, equal_dict[attr])\n",
    "        # 特殊属性替换\n",
    "        if '中长款' in title:\n",
    "            title = title.replace('中长款', '中款')\n",
    "        if '拉链' in title and '裤' in title and '无拉链' not in title:\n",
    "            title = title.replace('拉链', '拉链裤')\n",
    "        if '系带' in title and '裤' in title:\n",
    "            title = title.replace('系带', '系带裤')\n",
    "        if '松紧' in title and '裤' in title:\n",
    "            title = title.replace('松紧', '松紧裤')\n",
    "        if '拉链' in title and ('鞋' in title or '靴' in title):\n",
    "            title = title.replace('拉链', '拉链鞋')\n",
    "        if '系带' in title and ('鞋' in title or '靴' in title):\n",
    "            title = title.replace('系带', '系带鞋')\n",
    "        # 一个高频词的特殊处理\n",
    "        if '厚度常规' in title:\n",
    "            title = title.replace('厚度常规', '常规厚度')\n",
    "            \n",
    "        # 属性提取\n",
    "        for query in data['query']:\n",
    "            if query != '图文':\n",
    "                flag = 0\n",
    "                attr_list = attr_dict[query]\n",
    "                for attr in attr_list:\n",
    "                    if attr in title:\n",
    "                        key_attr[query] = attr  \n",
    "                        flag = 1\n",
    "                if flag == 0: # 检查有没有没对应上的query\n",
    "                    print(data['title'])\n",
    "                    print(data['query'])\n",
    "            \n",
    "        data['key_attr'] = key_attr\n",
    "        data['title'] = title\n",
    "        feature = data['feature']\n",
    "        del data['feature']\n",
    "        data['feature'] = feature\n",
    "        \n",
    "        rets.append(json.dumps(data, ensure_ascii=False)+'\\n')\n",
    "        \n",
    "        # if i>500:\n",
    "        #     break\n",
    "        # i += 1\n",
    "        \n",
    "print(len(rets))\n",
    "with open(test_save_file, 'w') as f:\n",
    "    f.writelines(rets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10000it [00:02, 3681.72it/s]\n"
     ]
    }
   ],
   "source": [
    "# 生成nofeat版本\n",
    "file = '../../data/equal_processed_data/test10000.txt'\n",
    "save_file = '../../data/equal_processed_data/nofeat/test10000.txt'\n",
    "\n",
    "rets = []\n",
    "with open(file, 'r') as f:\n",
    "    for i, line in enumerate(tqdm(f)):\n",
    "        item = json.loads(line)\n",
    "        del item['feature']\n",
    "        rets.append(json.dumps(item, ensure_ascii=False)+'\\n')\n",
    "        \n",
    "with open(save_file, 'w') as f:\n",
    "    f.writelines(rets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a7cf278074aea429de13d88b394304564a7b018f3e18e13089daf4fad90abe0d"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
