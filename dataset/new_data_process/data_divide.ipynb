{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm import tqdm \n",
    "import json\n",
    "import random \n",
    "import numpy as np \n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "seed = 0\n",
    "\n",
    "fine_file = '../../data/new_data/equal_split_word/fine50000.txt'\n",
    "coarse_neg_file = '../../data/new_data/equal_split_word/coarse10412.txt'\n",
    "\n",
    "SAVE_DIR = '../../data/new_data/divided/title/shuffle'\n",
    "if not os.path.exists(SAVE_DIR):\n",
    "    os.makedirs(SAVE_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "50000it [00:18, 2677.72it/s]\n"
     ]
    }
   ],
   "source": [
    "all_data = []\n",
    "with open(fine_file, 'r') as f:\n",
    "    for i, data in enumerate(tqdm(f)):\n",
    "        data = json.loads(data)\n",
    "        all_data.append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "divide fine data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "50000it [00:18, 2673.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35300\n",
      "5000\n",
      "9000\n",
      "700\n"
     ]
    }
   ],
   "source": [
    "# [fine] 划分train val数据\n",
    "print('divide fine data...')\n",
    "# name_list = ['fine35300.txt', 'fine5000.txt', 'fine9000.txt', 'fine700.txt']\n",
    "# name_list = ['coarse9000.txt', 'coarse1412']\n",
    "\n",
    "path1 = os.path.join(SAVE_DIR, 'fine35300.txt')\n",
    "path2 = os.path.join(SAVE_DIR, 'fine5000.txt')\n",
    "path3 = os.path.join(SAVE_DIR, 'fine9000.txt')\n",
    "path4 = os.path.join(SAVE_DIR, 'fine700.txt')\n",
    "\n",
    "rets1 = []\n",
    "rets2 = []\n",
    "rets3 = []\n",
    "rets4 = []\n",
    "\n",
    "all_data = []\n",
    "with open(fine_file, 'r') as f:\n",
    "    for i, data in enumerate(tqdm(f)):\n",
    "        data = json.loads(data)\n",
    "        all_data.append(data)\n",
    "        \n",
    "random.seed(seed)\n",
    "random.shuffle(all_data)\n",
    "\n",
    "for data in all_data:\n",
    "    if len(rets1) < 35300:\n",
    "        rets1.append(json.dumps(data, ensure_ascii=False)+'\\n')\n",
    "    elif len(rets2) < 5000:\n",
    "        rets2.append(json.dumps(data, ensure_ascii=False)+'\\n')\n",
    "    elif len(rets3) < 9000:\n",
    "        rets3.append(json.dumps(data, ensure_ascii=False)+'\\n')\n",
    "    elif len(rets4) < 700:\n",
    "        rets4.append(json.dumps(data, ensure_ascii=False)+'\\n')\n",
    "\n",
    "        \n",
    "print(len(rets1))\n",
    "print(len(rets2))\n",
    "print(len(rets3))\n",
    "print(len(rets4))\n",
    "\n",
    "with open(path1, 'w') as f:\n",
    "    f.writelines(rets1)\n",
    "with open(path2, 'w') as f:\n",
    "    f.writelines(rets2)\n",
    "with open(path3, 'w') as f:\n",
    "    f.writelines(rets3)\n",
    "with open(path4, 'w') as f:\n",
    "    f.writelines(rets4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "divide fine data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10412it [00:03, 2835.26it/s]\n",
      "100%|██████████| 10412/10412 [00:08<00:00, 1231.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9000\n",
      "1412\n"
     ]
    }
   ],
   "source": [
    "# [fine] 划分train val数据\n",
    "print('divide fine data...')\n",
    "# name_list = ['fine35300.txt', 'fine5000.txt', 'fine9000.txt', 'fine700.txt']\n",
    "# name_list = ['coarse9000.txt', 'coarse1412']\n",
    "\n",
    "path1 = os.path.join(SAVE_DIR, 'coarse9000.txt')\n",
    "path2 = os.path.join(SAVE_DIR, 'coarse1412.txt')\n",
    "\n",
    "rets1 = []\n",
    "rets2 = []\n",
    "\n",
    "all_data = []\n",
    "with open(coarse_neg_file, 'r') as f:\n",
    "    for i, data in enumerate(tqdm(f)):\n",
    "        data = json.loads(data)\n",
    "        all_data.append(data)\n",
    "        \n",
    "random.seed(seed)\n",
    "random.shuffle(all_data)\n",
    "\n",
    "for data in tqdm(all_data):\n",
    "    if len(rets1) < 9000:\n",
    "        rets1.append(json.dumps(data, ensure_ascii=False)+'\\n')\n",
    "    elif len(rets2) < 1412:\n",
    "        rets2.append(json.dumps(data, ensure_ascii=False)+'\\n')\n",
    "\n",
    "        \n",
    "print(len(rets1))\n",
    "print(len(rets2))\n",
    "\n",
    "with open(path1, 'w') as f:\n",
    "    f.writelines(rets1)\n",
    "with open(path2, 'w') as f:\n",
    "    f.writelines(rets2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [fine] 划分train val数据\n",
    "print('divide fine data...')\n",
    "fine_train_path = os.path.join(SAVE_DIR, 'fine45000.txt')\n",
    "fine_val_path = os.path.join(SAVE_DIR, 'fine5000.txt')\n",
    "\n",
    "train_rets = []\n",
    "val_rets = []\n",
    "\n",
    "all_data = []\n",
    "with open(fine_file, 'r') as f:\n",
    "    for i, data in enumerate(tqdm(f)):\n",
    "        data = json.loads(data)\n",
    "        all_data.append(data)\n",
    "        \n",
    "random.seed(seed)\n",
    "random.shuffle(all_data)\n",
    "\n",
    "for data in all_data:\n",
    "    if len(train_rets) < 45000:\n",
    "        train_rets.append(json.dumps(data, ensure_ascii=False)+'\\n')\n",
    "    else:\n",
    "        val_rets.append(json.dumps(data, ensure_ascii=False)+'\\n')\n",
    "        \n",
    "print(len(train_rets))\n",
    "print(len(val_rets))\n",
    "\n",
    "with open(fine_train_path, 'w') as f:\n",
    "    f.writelines(train_rets)\n",
    "with open(fine_val_path, 'w') as f:\n",
    "    f.writelines(val_rets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [fine] 划分train val数据\n",
    "print('divide fine data...')\n",
    "fine_train_path = os.path.join(SAVE_DIR, 'fine45000.txt')\n",
    "fine_val_path = os.path.join(SAVE_DIR, 'fine5000.txt')\n",
    "\n",
    "train_rets = []\n",
    "val_rets = []\n",
    "\n",
    "all_data = []\n",
    "with open(fine_file, 'r') as f:\n",
    "    for i, data in enumerate(tqdm(f)):\n",
    "        data = json.loads(data)\n",
    "        all_data.append(data)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('divide fine data...')\n",
    "\n",
    "kf = KFold(n_splits=10, shuffle=True, random_state=seed)\n",
    "for fold_id, (train_index, val_index) in enumerate(kf.split(all_data)):\n",
    "    print(fold_id)\n",
    "    train_rets = []\n",
    "    val_rets = []\n",
    "    \n",
    "    for idx in train_index:\n",
    "        train_rets.append(json.dumps(all_data[idx], ensure_ascii=False)+'\\n')\n",
    "    for idx in val_index:\n",
    "        val_rets.append(json.dumps(all_data[idx], ensure_ascii=False)+'\\n')\n",
    "\n",
    "    print(len(train_rets))\n",
    "    print(len(val_rets))\n",
    "\n",
    "    fine_train_path = os.path.join(SAVE_DIR, 'fine45000_'+str(fold_id)+'.txt')\n",
    "    fine_val_path = os.path.join(SAVE_DIR, 'fine5000_'+str(fold_id)+'.txt')\n",
    "    with open(fine_train_path, 'w') as f:\n",
    "        f.writelines(train_rets)\n",
    "    with open(fine_val_path, 'w') as f:\n",
    "        f.writelines(val_rets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5000it [00:18, 275.06it/s]\n",
      "5000it [00:18, 272.42it/s]\n",
      "5000it [00:18, 273.70it/s]\n",
      "5000it [00:18, 274.72it/s]\n",
      "5000it [00:18, 272.42it/s]\n",
      "5000it [00:18, 273.07it/s]\n",
      "5000it [00:18, 276.32it/s]\n",
      "5000it [00:18, 274.04it/s]\n",
      "5000it [00:18, 273.96it/s]\n",
      "5000it [00:18, 274.15it/s]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import torch \n",
    "import json\n",
    "import numpy as np \n",
    "from tqdm import tqdm \n",
    "import copy \n",
    "\n",
    "seed = 0\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "\n",
    "vocab_dict_file = '../../data/new_data/vocab/vocab_dict.json'\n",
    "vocab_file = '../../data/new_data/vocab/vocab.txt'\n",
    "relation_dict_file = '../../data/new_data/equal_processed_data/attr_relation_dict.json'\n",
    "\n",
    "\n",
    "with open(relation_dict_file, 'r') as f:\n",
    "    relation_dict = json.load(f)\n",
    "\n",
    "for fold_id in range(10):\n",
    "    val_file = '../../data/new_data/divided/attr/fine5000_'+str(fold_id)+'.txt'\n",
    "    save_file = '../../data/new_data/divided/attr/val/noposaug/fine5000_'+str(fold_id)+'.txt'\n",
    "    rets = []\n",
    "    with open(val_file, 'r') as f:\n",
    "        for line in tqdm(f):\n",
    "            item = json.loads(line)\n",
    "            if item['key_attr']: # 必须有属性\n",
    "                for query, attr in item['key_attr'].items():\n",
    "                    key_attr = {}\n",
    "                    match = {}\n",
    "                    new_item = copy.deepcopy(item)\n",
    "                    if random.random() < 0.5: # 替换，随机挑选一个词替换\n",
    "                        label = 0\n",
    "                        attr_list = random.sample(relation_dict[attr]['similar_attr'], 1)[0]\n",
    "                        if len(attr_list) == 1:\n",
    "                            split = attr_list\n",
    "                        else:\n",
    "                            attr = random.sample(attr_list, 1)[0]\n",
    "                            split = [attr]\n",
    "                    else: \n",
    "                        label = 1\n",
    "                        split = [attr]\n",
    "                        # if relation_dict[attr]['equal_attr']:\n",
    "                        #     if random.random() < 0.25: # 正例增强\n",
    "                        #         label = 1\n",
    "                        #         split = random.sample(relation_dict[attr]['equal_attr'], 1)\n",
    "                                \n",
    "                    key_attr[query] = split[0]\n",
    "                    match[query] = label\n",
    "                    new_item['key_attr'] = key_attr\n",
    "                    new_item['match'] = match\n",
    "                    rets.append(json.dumps(new_item, ensure_ascii=False)+'\\n')\n",
    "            \n",
    "    with open(save_file, 'w') as f:\n",
    "        f.writelines(rets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a7cf278074aea429de13d88b394304564a7b018f3e18e13089daf4fad90abe0d"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
