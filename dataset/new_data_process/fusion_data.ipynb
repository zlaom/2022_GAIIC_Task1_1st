{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import random\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "\n",
    "random.seed(0)\n",
    "# 直接从equal_split_word数据生成,考虑了coarse的情况\n",
    "attr_dict_file = '../../data/new_data/equal_processed_data/dict/attr_relation_dict.json'\n",
    "with open(attr_dict_file, 'r') as f:\n",
    "    relation_dict = json.load(f)\n",
    "\n",
    "# def get_negative_dict(attr_dict):\n",
    "#         negative_dict = {}\n",
    "#         for query, attr_list in attr_dict.items():\n",
    "#             negative_dict[query] = {}\n",
    "#             for attr in attr_list:\n",
    "#                 l = attr_list.copy()\n",
    "#                 l.remove(attr)\n",
    "#                 negative_dict[query][attr] = l\n",
    "#         return negative_dict\n",
    "# negative_dict = get_negative_dict(attr_dict)\n",
    "\n",
    "def generate_negative_data(input_data):\n",
    "    rets = []\n",
    "    for split_data in input_data:\n",
    "        # print(len(split_data))\n",
    "        split_rets = []\n",
    "        for data in split_data:\n",
    "            title = data['title']\n",
    "            key_attr = data['key_attr']\n",
    "            if not key_attr:\n",
    "                continue\n",
    "            match = data['match']\n",
    "            vocab_split = data['vocab_split']\n",
    "            del data['title_split']\n",
    "            match['图文'] = 0\n",
    "            # 先随机选取一个属性为负\n",
    "            query = random.sample(list(key_attr.keys()), 1)[0]\n",
    "            attr = key_attr[query]\n",
    "            attr_list = random.sample(relation_dict[attr]['similar_attr'], 1)[0]\n",
    "            if len(attr_list) == 1:\n",
    "                new_attr = attr_list[0]\n",
    "            else:\n",
    "                new_attr = random.sample(attr_list, 1)[0]\n",
    "            key_attr[query] = new_attr\n",
    "            title = title.replace(attr, new_attr)\n",
    "            idx = vocab_split.index(attr)\n",
    "            vocab_split[idx] = new_attr\n",
    "            match[query] = 0\n",
    "            # 其余属性以一定概率变负\n",
    "            for query, attr in key_attr.items():\n",
    "                if match[query] != 0:\n",
    "                    if random.random() < 0.4375: # 0.434\n",
    "                        attr_list = random.sample(relation_dict[attr]['similar_attr'], 1)[0]\n",
    "                        if len(attr_list) == 1:\n",
    "                            new_attr = attr_list[0]\n",
    "                        else:\n",
    "                            new_attr = random.sample(attr_list, 1)[0]\n",
    "                        key_attr[query] = new_attr\n",
    "                        title = title.replace(attr, new_attr)\n",
    "                        idx = vocab_split.index(attr)\n",
    "                        vocab_split[idx] = new_attr\n",
    "                        match[query] = 0\n",
    "            # 隐藏属性\n",
    "            if len(key_attr) >= 2: # 属性不少于两个\n",
    "                flag_0 = 0\n",
    "                flag_1 = 0\n",
    "                for query, value in match.items(): # 有正有负\n",
    "                    if query != '图文':\n",
    "                        if value == 1:\n",
    "                            flag_1 = 1\n",
    "                        else:\n",
    "                            flag_0 = 1\n",
    "                if flag_0 and flag_1:\n",
    "                    if random.random() < 0.3: # 一定概率进行隐藏\n",
    "                        match_copy = match.copy()\n",
    "                        for query, value in match_copy.items():\n",
    "                            if query != '图文':\n",
    "                                if value == 0:\n",
    "                                    del match[query]\n",
    "                                    del key_attr[query]\n",
    "            data['title'] = title \n",
    "            data['key_attr'] = key_attr \n",
    "            data['match'] = match \n",
    "            data['vocab_split'] = vocab_split\n",
    "            split_rets.append(data)\n",
    "        rets.append(split_rets)\n",
    "    return rets\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_pos_data(input_data):\n",
    "    rets = []\n",
    "    for data in input_data:\n",
    "        title = data['title']\n",
    "        key_attr = data['key_attr']\n",
    "        if not key_attr:\n",
    "            continue\n",
    "        match = data['match']\n",
    "        vocab_split = data['vocab_split']\n",
    "        del data['title_split']\n",
    "        \n",
    "        for query, attr in key_attr.items():\n",
    "            if relation_dict[attr]['equal_attr']:\n",
    "                if random.random() < 0.25: # 正例增强\n",
    "                    new_attr = random.sample(relation_dict[attr]['equal_attr'], 1)[0]\n",
    "                    key_attr[query] = new_attr\n",
    "                    title = title.replace(attr, new_attr)\n",
    "                    match[query] = 0.9\n",
    "                    idx = vocab_split.index(attr)\n",
    "                    vocab_split[idx] = new_attr\n",
    "                    \n",
    "        data['title'] = title \n",
    "        data['key_attr'] = key_attr \n",
    "        data['match'] = match \n",
    "        data['vocab_split'] = vocab_split\n",
    "        rets.append(data)\n",
    "    return rets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5000it [00:01, 2951.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos_index:0 neg_index:[1, 2, 3]\n",
      "pos_len:1250 neg_len:1250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5000it [00:01, 2924.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos_index:1 neg_index:[0, 2, 3]\n",
      "pos_len:1250 neg_len:1250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5000it [00:01, 2555.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos_index:2 neg_index:[0, 1, 3]\n",
      "pos_len:1250 neg_len:1250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5000it [00:01, 3261.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos_index:3 neg_index:[0, 1, 2]\n",
      "pos_len:1250 neg_len:1250\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import copy\n",
    "\n",
    "all_index = [0,1,2,3]\n",
    "\n",
    "for pos_index in all_index:\n",
    "    \n",
    "    data_file = \"../../data/new_data/divided/title/shuffle/fine5000.txt\"\n",
    "    origin_data = []\n",
    "    with open(data_file, 'r') as f:\n",
    "        for line in tqdm(f):\n",
    "            item = json.loads(line)\n",
    "            origin_data.append(item)\n",
    "            \n",
    "            \n",
    "    split_data  = []\n",
    "    item_len = len(origin_data)//4\n",
    "    for i in range(0, len(origin_data), item_len):\n",
    "        split_data.append(origin_data[i:i+item_len])\n",
    "        \n",
    "    split_data = np.array(split_data)\n",
    "    reslut = np.array([None]*4)\n",
    "    neg_index = all_index.copy()\n",
    "    neg_index.remove(pos_index)\n",
    "    print(f\"pos_index:{pos_index} neg_index:{neg_index}\")\n",
    "    \n",
    "    pos_data = split_data[pos_index]\n",
    "    neg_data = split_data[neg_index]\n",
    "    \n",
    "    print(f\"pos_len:{len(pos_data)} neg_len:{len(neg_data[0])}\")\n",
    "    \n",
    "    \n",
    "    pos_data = generate_pos_data(pos_data)\n",
    "    neg_data = generate_negative_data(neg_data)\n",
    "    \n",
    "    reslut[pos_index] = pos_data\n",
    "    reslut[neg_index] = neg_data\n",
    "    \n",
    "    save_result = []\n",
    "    for split_data in reslut:\n",
    "        for data in split_data:\n",
    "            save_result.append(json.dumps(data, ensure_ascii=False)+'\\n')\n",
    "    \n",
    "    save_path = os.path.join('../../output/fusion', f\"pos_{pos_index}.txt\")\n",
    "    with open(save_path, 'w') as f:\n",
    "        f.writelines(save_result)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5000it [00:01, 3482.14it/s]\n",
      "5000it [00:01, 3462.10it/s]\n",
      "5000it [00:01, 3499.42it/s]\n",
      "5000it [00:01, 3465.56it/s]\n"
     ]
    }
   ],
   "source": [
    "# no feat\n",
    "for pos_index in range(4):\n",
    "    save_path = os.path.join('../../output/fusion', f\"pos_{pos_index}.txt\")\n",
    "    \n",
    "    all_data = []\n",
    "    with open(save_path, 'r') as f:\n",
    "        for line in tqdm(f):\n",
    "            data = json.loads(line)\n",
    "            del data[\"feature\"]\n",
    "            all_data.append(json.dumps(data, ensure_ascii=False)+'\\n')\n",
    "\n",
    "    save_path = os.path.join('../../output/fusion/no_feat', f\"pos_{pos_index}.txt\")\n",
    "    with open(save_path, 'w') as f:\n",
    "        f.writelines(all_data)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a7cf278074aea429de13d88b394304564a7b018f3e18e13089daf4fad90abe0d"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
