{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "139588 10412 139588\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "import re\n",
    "import numpy as np\n",
    "import json\n",
    "import random\n",
    "import copy\n",
    "\n",
    "data_path_1 = '../data/train_fine.txt'\n",
    "data_path_2 = '../data/train_coarse.txt'\n",
    "\n",
    "data_pos_list = []\n",
    "data_neg_list = []\n",
    "\n",
    "with open(data_path_1, 'r', encoding='utf-8') as f:\n",
    "    lines = f.readlines()\n",
    "    for line in lines:\n",
    "        data = json.loads(line)\n",
    "        title = data['title']\n",
    "        title = re.sub(r'\\d{1,4}年','', title)\n",
    "        data['title'] = title\n",
    "        if data['match']['图文'] == 1:\n",
    "            data_pos_list.append(data)\n",
    "        else:\n",
    "            data_neg_list.append(data)\n",
    "\n",
    "with open(data_path_2, 'r', encoding='utf-8') as f:\n",
    "    lines = f.readlines()\n",
    "    for line in lines:\n",
    "        data = json.loads(line)\n",
    "        title = data['title']\n",
    "        title = re.sub(r'\\d{1,4}年','', title)\n",
    "        data['title'] = title\n",
    "        if data['match']['图文'] == 1:\n",
    "            data_pos_list.append(data)\n",
    "        else:\n",
    "            data_neg_list.append(data)\n",
    "\n",
    "new_neg_list = []\n",
    "for i in range(len(data_pos_list)):\n",
    "    dic = copy.deepcopy(data_pos_list[i])\n",
    "    index = random.randint(0, len(data_pos_list)-1)\n",
    "    while index == i:\n",
    "        index = random.randint(0, len(data_pos_list)-1)\n",
    "    dic['title'] = data_pos_list[index]['title']\n",
    "    dic['match']['图文'] = 0\n",
    "    new_neg_list.append(dic)\n",
    "\n",
    "print(len(data_pos_list), len(data_neg_list), len(new_neg_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/zlm/workspace/GAIIC_2022_1/title_image_baseline/data_process.ipynb Cell 9'\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bd4_env_zlm/home/zlm/workspace/GAIIC_2022_1/title_image_baseline/data_process.ipynb#ch0000008vscode-remote?line=5'>6</a>\u001b[0m \u001b[39m# np.random.shuffle(x_val_list)\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bd4_env_zlm/home/zlm/workspace/GAIIC_2022_1/title_image_baseline/data_process.ipynb#ch0000008vscode-remote?line=7'>8</a>\u001b[0m x_train_list \u001b[39m=\u001b[39m [json\u001b[39m.\u001b[39mdumps(dic, ensure_ascii\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\u001b[39m+\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m'\u001b[39m \u001b[39mfor\u001b[39;00m dic \u001b[39min\u001b[39;00m x_train_list]\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bd4_env_zlm/home/zlm/workspace/GAIIC_2022_1/title_image_baseline/data_process.ipynb#ch0000008vscode-remote?line=8'>9</a>\u001b[0m x_val_list \u001b[39m=\u001b[39m [json\u001b[39m.\u001b[39mdumps(dic, ensure_ascii\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\u001b[39m+\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m'\u001b[39m \u001b[39mfor\u001b[39;00m dic \u001b[39min\u001b[39;00m x_val_list]\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bd4_env_zlm/home/zlm/workspace/GAIIC_2022_1/title_image_baseline/data_process.ipynb#ch0000008vscode-remote?line=11'>12</a>\u001b[0m \u001b[39m# with open('./data/pretrain_match.txt', 'w', encoding='utf-8') as f:\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bd4_env_zlm/home/zlm/workspace/GAIIC_2022_1/title_image_baseline/data_process.ipynb#ch0000008vscode-remote?line=12'>13</a>\u001b[0m \u001b[39m#     f.writelines(pre_ret)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bd4_env_zlm/home/zlm/workspace/GAIIC_2022_1/title_image_baseline/data_process.ipynb#ch0000008vscode-remote?line=14'>15</a>\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(\u001b[39m'\u001b[39m\u001b[39m../data/preprocessed_data/new_train_match.txt\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mw\u001b[39m\u001b[39m'\u001b[39m, encoding\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mas\u001b[39;00m f:\n",
      "\u001b[1;32m/home/zlm/workspace/GAIIC_2022_1/title_image_baseline/data_process.ipynb Cell 9'\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bd4_env_zlm/home/zlm/workspace/GAIIC_2022_1/title_image_baseline/data_process.ipynb#ch0000008vscode-remote?line=5'>6</a>\u001b[0m \u001b[39m# np.random.shuffle(x_val_list)\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bd4_env_zlm/home/zlm/workspace/GAIIC_2022_1/title_image_baseline/data_process.ipynb#ch0000008vscode-remote?line=7'>8</a>\u001b[0m x_train_list \u001b[39m=\u001b[39m [json\u001b[39m.\u001b[39mdumps(dic, ensure_ascii\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\u001b[39m+\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m'\u001b[39m \u001b[39mfor\u001b[39;00m dic \u001b[39min\u001b[39;00m x_train_list]\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bd4_env_zlm/home/zlm/workspace/GAIIC_2022_1/title_image_baseline/data_process.ipynb#ch0000008vscode-remote?line=8'>9</a>\u001b[0m x_val_list \u001b[39m=\u001b[39m [json\u001b[39m.\u001b[39;49mdumps(dic, ensure_ascii\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\u001b[39m+\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m'\u001b[39m \u001b[39mfor\u001b[39;00m dic \u001b[39min\u001b[39;00m x_val_list]\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bd4_env_zlm/home/zlm/workspace/GAIIC_2022_1/title_image_baseline/data_process.ipynb#ch0000008vscode-remote?line=11'>12</a>\u001b[0m \u001b[39m# with open('./data/pretrain_match.txt', 'w', encoding='utf-8') as f:\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bd4_env_zlm/home/zlm/workspace/GAIIC_2022_1/title_image_baseline/data_process.ipynb#ch0000008vscode-remote?line=12'>13</a>\u001b[0m \u001b[39m#     f.writelines(pre_ret)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bd4_env_zlm/home/zlm/workspace/GAIIC_2022_1/title_image_baseline/data_process.ipynb#ch0000008vscode-remote?line=14'>15</a>\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(\u001b[39m'\u001b[39m\u001b[39m../data/preprocessed_data/new_train_match.txt\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mw\u001b[39m\u001b[39m'\u001b[39m, encoding\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mas\u001b[39;00m f:\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/json/__init__.py:234\u001b[0m, in \u001b[0;36mdumps\u001b[0;34m(obj, skipkeys, ensure_ascii, check_circular, allow_nan, cls, indent, separators, default, sort_keys, **kw)\u001b[0m\n\u001b[1;32m    <a href='file:///home/zlm/miniconda3/lib/python3.9/json/__init__.py?line=231'>232</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mcls\u001b[39m \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    <a href='file:///home/zlm/miniconda3/lib/python3.9/json/__init__.py?line=232'>233</a>\u001b[0m     \u001b[39mcls\u001b[39m \u001b[39m=\u001b[39m JSONEncoder\n\u001b[0;32m--> <a href='file:///home/zlm/miniconda3/lib/python3.9/json/__init__.py?line=233'>234</a>\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mcls\u001b[39;49m(\n\u001b[1;32m    <a href='file:///home/zlm/miniconda3/lib/python3.9/json/__init__.py?line=234'>235</a>\u001b[0m     skipkeys\u001b[39m=\u001b[39;49mskipkeys, ensure_ascii\u001b[39m=\u001b[39;49mensure_ascii,\n\u001b[1;32m    <a href='file:///home/zlm/miniconda3/lib/python3.9/json/__init__.py?line=235'>236</a>\u001b[0m     check_circular\u001b[39m=\u001b[39;49mcheck_circular, allow_nan\u001b[39m=\u001b[39;49mallow_nan, indent\u001b[39m=\u001b[39;49mindent,\n\u001b[1;32m    <a href='file:///home/zlm/miniconda3/lib/python3.9/json/__init__.py?line=236'>237</a>\u001b[0m     separators\u001b[39m=\u001b[39;49mseparators, default\u001b[39m=\u001b[39;49mdefault, sort_keys\u001b[39m=\u001b[39;49msort_keys,\n\u001b[1;32m    <a href='file:///home/zlm/miniconda3/lib/python3.9/json/__init__.py?line=237'>238</a>\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkw)\u001b[39m.\u001b[39;49mencode(obj)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/json/encoder.py:199\u001b[0m, in \u001b[0;36mJSONEncoder.encode\u001b[0;34m(self, o)\u001b[0m\n\u001b[1;32m    <a href='file:///home/zlm/miniconda3/lib/python3.9/json/encoder.py?line=194'>195</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m encode_basestring(o)\n\u001b[1;32m    <a href='file:///home/zlm/miniconda3/lib/python3.9/json/encoder.py?line=195'>196</a>\u001b[0m \u001b[39m# This doesn't pass the iterator directly to ''.join() because the\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/zlm/miniconda3/lib/python3.9/json/encoder.py?line=196'>197</a>\u001b[0m \u001b[39m# exceptions aren't as detailed.  The list call should be roughly\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/zlm/miniconda3/lib/python3.9/json/encoder.py?line=197'>198</a>\u001b[0m \u001b[39m# equivalent to the PySequence_Fast that ''.join() would do.\u001b[39;00m\n\u001b[0;32m--> <a href='file:///home/zlm/miniconda3/lib/python3.9/json/encoder.py?line=198'>199</a>\u001b[0m chunks \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49miterencode(o, _one_shot\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m    <a href='file:///home/zlm/miniconda3/lib/python3.9/json/encoder.py?line=199'>200</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(chunks, (\u001b[39mlist\u001b[39m, \u001b[39mtuple\u001b[39m)):\n\u001b[1;32m    <a href='file:///home/zlm/miniconda3/lib/python3.9/json/encoder.py?line=200'>201</a>\u001b[0m     chunks \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(chunks)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/json/encoder.py:257\u001b[0m, in \u001b[0;36mJSONEncoder.iterencode\u001b[0;34m(self, o, _one_shot)\u001b[0m\n\u001b[1;32m    <a href='file:///home/zlm/miniconda3/lib/python3.9/json/encoder.py?line=251'>252</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    <a href='file:///home/zlm/miniconda3/lib/python3.9/json/encoder.py?line=252'>253</a>\u001b[0m     _iterencode \u001b[39m=\u001b[39m _make_iterencode(\n\u001b[1;32m    <a href='file:///home/zlm/miniconda3/lib/python3.9/json/encoder.py?line=253'>254</a>\u001b[0m         markers, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdefault, _encoder, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindent, floatstr,\n\u001b[1;32m    <a href='file:///home/zlm/miniconda3/lib/python3.9/json/encoder.py?line=254'>255</a>\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mkey_separator, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitem_separator, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msort_keys,\n\u001b[1;32m    <a href='file:///home/zlm/miniconda3/lib/python3.9/json/encoder.py?line=255'>256</a>\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mskipkeys, _one_shot)\n\u001b[0;32m--> <a href='file:///home/zlm/miniconda3/lib/python3.9/json/encoder.py?line=256'>257</a>\u001b[0m \u001b[39mreturn\u001b[39;00m _iterencode(o, \u001b[39m0\u001b[39;49m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "all_data = data_pos_list + data_neg_list + new_neg_list\n",
    "np.random.shuffle(all_data)\n",
    "l = int(len(all_data) * 0.9)\n",
    "x_train_list = all_data[:l]\n",
    "x_val_list = all_data[l:]\n",
    "# np.random.shuffle(x_val_list)\n",
    "\n",
    "x_train_list = [json.dumps(dic, ensure_ascii=False)+'\\n' for dic in x_train_list]\n",
    "x_val_list = [json.dumps(dic, ensure_ascii=False)+'\\n' for dic in x_val_list]\n",
    "\n",
    "\n",
    "# with open('./data/pretrain_match.txt', 'w', encoding='utf-8') as f:\n",
    "#     f.writelines(pre_ret)\n",
    "\n",
    "with open('../data/preprocessed_data/new_train_match.txt', 'w', encoding='utf-8') as f:\n",
    "    f.writelines(x_train_list)\n",
    "\n",
    "with open('../data/preprocessed_data/new_val_match.txt', 'w', encoding='utf-8') as f:\n",
    "    f.writelines(x_val_list)\n",
    "\n",
    "# print(len(pre_ret))\n",
    "print(len(x_train_list))\n",
    "print(len(x_val_list))\n",
    "print(len(x_train_list) + len(x_val_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'领型': [['高领', '半高领', '立领'], ['连帽', '可脱卸帽'], ['翻领', '衬衫领', 'POLO领', '方领', '娃娃领', '荷叶领'], ['双层领'], ['西装领'], ['U型领'], ['一字领'], ['围巾领'], ['堆堆领'], ['V领'], ['棒球领'], ['圆领'], ['斜领'], ['亨利领']], '袖长': [['短袖', '五分袖'], ['九分袖', '长袖'], ['七分袖'], ['无袖']], '衣长': [['超短款', '短款', '常规款'], ['长款', '超长款'], ['中长款']], '版型': [['修身型', '标准型'], ['宽松型']], '裙长': [['短裙', '超短裙'], ['中裙', '中长裙'], ['长裙']], '穿着方式': [['套头'], ['开衫']], '类别': [['手提包'], ['单肩包'], ['斜挎包'], ['双肩包']], '裤型': [['O型裤', '锥形裤', '哈伦裤', '灯笼裤'], ['铅笔裤', '直筒裤', '小脚裤'], ['工装裤'], ['紧身裤'], ['背带裤'], ['喇叭裤', '微喇裤'], ['阔腿裤']], '裤长': [['短裤'], ['五分裤'], ['七分裤'], ['九分裤', '长裤']], '裤门襟': [['松紧'], ['拉链'], ['系带']], '闭合方式': [['松紧带'], ['拉链'], ['套筒', '套脚', '一脚蹬'], ['系带'], ['魔术贴'], ['搭扣']], '鞋帮高度': [['高帮', '中帮'], ['低帮']]}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "new_dic = {}\n",
    "\n",
    "with open('../data/attr_to_attrvals.json', 'r', encoding='utf-8') as f:\n",
    "    attr_key = json.load(f)\n",
    "    for key, value in attr_key.items():\n",
    "        tmp = []\n",
    "        for v in value:\n",
    "            if '=' in v:\n",
    "                tmp.append(v.split('='))\n",
    "            else:\n",
    "                tmp.append([v])\n",
    "        new_dic[key] = tmp\n",
    "print(new_dic)\n",
    "rets = [json.dumps(new_dic, ensure_ascii=False)+'\\n']\n",
    "with open('../data/preprocessed_data/attr_match.json', 'w', encoding='utf-8') as f:\n",
    "    f.writelines(rets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "key type: 14 num:22\n",
      "key type: 4 num:6\n",
      "key type: 3 num:6\n",
      "key type: 2 num:3\n",
      "key type: 3 num:5\n",
      "key type: 2 num:2\n",
      "key type: 4 num:4\n",
      "key type: 7 num:13\n",
      "key type: 4 num:5\n",
      "key type: 3 num:3\n",
      "key type: 6 num:8\n",
      "key type: 2 num:3\n"
     ]
    }
   ],
   "source": [
    "with open('../data/preprocessed_data/attr_match.json', 'r', encoding='utf-8') as f:\n",
    "    attr_key = json.load(f)\n",
    "\n",
    "for key, value in attr_key.items():\n",
    "    att = set()\n",
    "    for i in value:\n",
    "        for j in i:\n",
    "            att.add(j)\n",
    "    print(f'key type: {len(value)} num:{len(att)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/zlm/workspace/GAIIC_2022_1/title_image_baseline/data_process.ipynb Cell 12'\u001b[0m in \u001b[0;36m<cell line: 83>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bd4_env_zlm/home/zlm/workspace/GAIIC_2022_1/title_image_baseline/data_process.ipynb#ch0000010vscode-remote?line=88'>89</a>\u001b[0m new_item \u001b[39m=\u001b[39m get_pos_item(data, attr_key)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bd4_env_zlm/home/zlm/workspace/GAIIC_2022_1/title_image_baseline/data_process.ipynb#ch0000010vscode-remote?line=89'>90</a>\u001b[0m new_item \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m math_list\u001b[39m.\u001b[39mappend(json\u001b[39m.\u001b[39mdumps(new_item, ensure_ascii\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\u001b[39m+\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bd4_env_zlm/home/zlm/workspace/GAIIC_2022_1/title_image_baseline/data_process.ipynb#ch0000010vscode-remote?line=90'>91</a>\u001b[0m new_item \u001b[39m=\u001b[39m get_neg_item(new_item, attr_key)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bd4_env_zlm/home/zlm/workspace/GAIIC_2022_1/title_image_baseline/data_process.ipynb#ch0000010vscode-remote?line=91'>92</a>\u001b[0m new_item \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m not_math_list\u001b[39m.\u001b[39mappend(json\u001b[39m.\u001b[39mdumps(new_item, ensure_ascii\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\u001b[39m+\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bd4_env_zlm/home/zlm/workspace/GAIIC_2022_1/title_image_baseline/data_process.ipynb#ch0000010vscode-remote?line=92'>93</a>\u001b[0m \u001b[39m# new_item = get_neg_item(data, attr_key)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bd4_env_zlm/home/zlm/workspace/GAIIC_2022_1/title_image_baseline/data_process.ipynb#ch0000010vscode-remote?line=93'>94</a>\u001b[0m \u001b[39m# new_item is not None and not_math_list.append(json.dumps(new_item, ensure_ascii=False)+'\\n')\u001b[39;00m\n",
      "\u001b[1;32m/home/zlm/workspace/GAIIC_2022_1/title_image_baseline/data_process.ipynb Cell 12'\u001b[0m in \u001b[0;36mget_neg_item\u001b[0;34m(ori_item, attr_key)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bd4_env_zlm/home/zlm/workspace/GAIIC_2022_1/title_image_baseline/data_process.ipynb#ch0000010vscode-remote?line=7'>8</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_neg_item\u001b[39m(ori_item, attr_key):\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bd4_env_zlm/home/zlm/workspace/GAIIC_2022_1/title_image_baseline/data_process.ipynb#ch0000010vscode-remote?line=8'>9</a>\u001b[0m     new_item \u001b[39m=\u001b[39m copy\u001b[39m.\u001b[39mdeepcopy(ori_item)\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bd4_env_zlm/home/zlm/workspace/GAIIC_2022_1/title_image_baseline/data_process.ipynb#ch0000010vscode-remote?line=9'>10</a>\u001b[0m     keys \u001b[39m=\u001b[39m [x \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m new_item[\u001b[39m'\u001b[39;49m\u001b[39mmatch\u001b[39;49m\u001b[39m'\u001b[39;49m]\u001b[39m.\u001b[39mkeys() \u001b[39mif\u001b[39;00m x \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m图文\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bd4_env_zlm/home/zlm/workspace/GAIIC_2022_1/title_image_baseline/data_process.ipynb#ch0000010vscode-remote?line=10'>11</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(keys) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bd4_env_zlm/home/zlm/workspace/GAIIC_2022_1/title_image_baseline/data_process.ipynb#ch0000010vscode-remote?line=11'>12</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from random import choice\n",
    "\n",
    "with open('../data/preprocessed_data/attr_match.json', 'r', encoding='utf-8') as f:\n",
    "    attr_key = json.load(f)\n",
    "\n",
    "def get_neg_item(ori_item, attr_key):\n",
    "    new_item = copy.deepcopy(ori_item)\n",
    "    keys = [x for x in new_item['match'].keys() if x != '图文']\n",
    "    if len(keys) == 0:\n",
    "        return None\n",
    "    change_num = 0\n",
    "    rates = (np.random.random(len(keys))>0.7)\n",
    "    while np.sum(rates)==0:\n",
    "        rates = (np.random.random(len(keys))>0.7)\n",
    "    for key, rate in zip(keys,rates):\n",
    "        if rate>0.6:\n",
    "            all_key_values = attr_key[key]\n",
    "            ori_value = new_item['key_attr'][key]\n",
    "            key_index = 0\n",
    "            for i in range(len(all_key_values)):\n",
    "                if ori_value in all_key_values[i]:\n",
    "                    key_index = i\n",
    "                    break\n",
    "            new_index = np.random.randint(len(all_key_values))\n",
    "            while new_index == key_index:\n",
    "                new_index = np.random.randint(len(all_key_values))\n",
    "            sub_val = all_key_values[new_index]\n",
    "            new_value = sub_val[np.random.randint(len(sub_val))]\n",
    "            new_item['title'] = new_item['title'].replace(ori_value, new_value, 1)\n",
    "            new_item['key_attr'][key] = new_value\n",
    "            new_item['match'][key] = 0\n",
    "            change_num+=1\n",
    "    if change_num>0:\n",
    "        new_item['match']['图文'] = 0\n",
    "        return new_item\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def get_pos_item(ori_item, attr_key):\n",
    "    new_item = copy.deepcopy(ori_item)\n",
    "    keys = [x for x in new_item['match'].keys() if x != '图文']\n",
    "    if len(keys) == 0:\n",
    "        return None\n",
    "    change_num = 0\n",
    "    for key in keys:\n",
    "        all_key_values = attr_key[key]\n",
    "        # print(keys, key, new_item['key_attr'])\n",
    "        ori_value = new_item['key_attr'][key]\n",
    "        key_index = 0\n",
    "        for i in range(len(all_key_values)):\n",
    "            if ori_value in all_key_values[i]:\n",
    "                key_index = i\n",
    "                break\n",
    "        sim_key_values = all_key_values[key_index]\n",
    "        if len(sim_key_values)>1:\n",
    "            new_value = ori_value\n",
    "            while new_value == ori_value:\n",
    "                new_value = sim_key_values[np.random.randint(len(sim_key_values))]\n",
    "            new_item['title'] = new_item['title'].replace(ori_value, new_value, 1)\n",
    "            new_item['key_attr'][key] = new_value\n",
    "            change_num+=1\n",
    "    if change_num>0:\n",
    "        return new_item\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "\n",
    "    \n",
    "# def get_random_key(keys, ratio=0.7):\n",
    "#     list_ratio = [0.7, 0.9, 1]\n",
    "#     ratio = choice(list_ratio)\n",
    "#     l = int(len(keys) * ratio)\n",
    "#     if l == 0:\n",
    "#         l = 1\n",
    "#     np.random.shuffle(keys)\n",
    "#     return keys[:l]\n",
    "\n",
    "count = 0\n",
    "math_list = []\n",
    "not_math_list = []\n",
    "with open('../data/train_fine.txt', 'r', encoding='utf-8') as f:\n",
    "    lines = f.readlines()\n",
    "    new_dic = {}\n",
    "    for line in lines:\n",
    "        data = json.loads(line)\n",
    "        # math_list.append(json.dumps(data, ensure_ascii=False)+'\\n')\n",
    "        new_item = get_pos_item(data, attr_key)\n",
    "        new_item is not None and math_list.append(json.dumps(new_item, ensure_ascii=False)+'\\n')\n",
    "        new_item = get_neg_item(data, attr_key)\n",
    "        new_item is not None and not_math_list.append(json.dumps(new_item, ensure_ascii=False)+'\\n')\n",
    "        # new_item = get_neg_item(data, attr_key)\n",
    "        # new_item is not None and not_math_list.append(json.dumps(new_item, ensure_ascii=False)+'\\n')\n",
    "        count+=1\n",
    "        if count >20:\n",
    "            break\n",
    "\n",
    "with open('../data/preprocessed_data/my_pos_fine.txt', 'w', encoding='utf-8') as f:\n",
    "    f.writelines(math_list)\n",
    "with open('../data/preprocessed_data/my_neg_fine.txt', 'w', encoding='utf-8') as f:\n",
    "    f.writelines(not_math_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_34112/30112977.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0mnew_item\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_uni_pos_item\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattr_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0mnew_item\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mmath_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_item\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mensure_ascii\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m         \u001b[0mnew_item\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_neg_item\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_item\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattr_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m         \u001b[0mnew_item\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mnot_math_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_item\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mensure_ascii\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0;31m# new_item = get_neg_item(data, attr_key)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_34112/30112977.py\u001b[0m in \u001b[0;36mget_neg_item\u001b[0;34m(ori_item, attr_key)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_neg_item\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mori_item\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattr_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mnew_item\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mori_item\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mkeys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnew_item\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'match'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'图文'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mae/lib/python3.8/copy.py\u001b[0m in \u001b[0;36mdeepcopy\u001b[0;34m(x, memo, _nil)\u001b[0m\n\u001b[1;32m    144\u001b[0m     \u001b[0mcopier\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_deepcopy_dispatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcopier\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0missubclass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mae/lib/python3.8/copy.py\u001b[0m in \u001b[0;36m_deepcopy_dict\u001b[0;34m(x, memo, deepcopy)\u001b[0m\n\u001b[1;32m    228\u001b[0m     \u001b[0mmemo\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 230\u001b[0;31m         \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    231\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_deepcopy_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mae/lib/python3.8/copy.py\u001b[0m in \u001b[0;36mdeepcopy\u001b[0;34m(x, memo, _nil)\u001b[0m\n\u001b[1;32m    144\u001b[0m     \u001b[0mcopier\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_deepcopy_dispatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcopier\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0missubclass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mae/lib/python3.8/copy.py\u001b[0m in \u001b[0;36m_deepcopy_list\u001b[0;34m(x, memo, deepcopy)\u001b[0m\n\u001b[1;32m    203\u001b[0m     \u001b[0mappend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 205\u001b[0;31m         \u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    206\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_deepcopy_list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mae/lib/python3.8/copy.py\u001b[0m in \u001b[0;36mdeepcopy\u001b[0;34m(x, memo, _nil)\u001b[0m\n\u001b[1;32m    144\u001b[0m     \u001b[0mcopier\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_deepcopy_dispatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcopier\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0missubclass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import copy\n",
    "import re\n",
    "from random import choice\n",
    "\n",
    "with open('../data/preprocessed_data/attr_match.json', 'r', encoding='utf-8') as f:\n",
    "    attr_key = json.load(f)\n",
    "\n",
    "def get_neg_item(ori_item, attr_key):\n",
    "    new_item = copy.deepcopy(ori_item)\n",
    "    keys = [x for x in new_item['match'].keys() if x != '图文']\n",
    "    if len(keys) == 0:\n",
    "        return None\n",
    "    change_num = 0\n",
    "    rates = (np.random.random(len(keys))>0.7)\n",
    "    while np.sum(rates)==0:\n",
    "        rates = (np.random.random(len(keys))>0.7)\n",
    "    for key, rate in zip(keys,rates):\n",
    "        if rate:\n",
    "            all_key_values = attr_key[key]\n",
    "            ori_value = new_item['key_attr'][key]\n",
    "            key_index = 0\n",
    "            for i in range(len(all_key_values)):\n",
    "                if ori_value in all_key_values[i]:\n",
    "                    key_index = i\n",
    "                    break\n",
    "            new_index = np.random.randint(len(all_key_values))\n",
    "            while new_index == key_index:\n",
    "                new_index = np.random.randint(len(all_key_values))\n",
    "            sub_val = all_key_values[new_index]\n",
    "            new_value = sub_val[0]\n",
    "            new_item['title'] = new_item['title'].replace(ori_value, new_value, 1)\n",
    "            new_item['key_attr'][key] = new_value\n",
    "            new_item['match'][key] = 0\n",
    "            change_num+=1\n",
    "    if change_num>0:\n",
    "        new_item['match']['图文'] = 0\n",
    "        return new_item\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def get_uni_pos_item(ori_item, attr_key):\n",
    "    new_item = copy.deepcopy(ori_item)\n",
    "    keys = [x for x in new_item['match'].keys() if x != '图文']\n",
    "    new_item['title'] = re.sub(r'\\d{1,4}年','', new_item['title'])\n",
    "    if len(keys) != 0:\n",
    "        change_num = 0\n",
    "        for key in keys:\n",
    "            all_key_values = attr_key[key]\n",
    "            ori_value = new_item['key_attr'][key]\n",
    "            key_index = 0\n",
    "            for i in range(len(all_key_values)):\n",
    "                if ori_value in all_key_values[i]:\n",
    "                    key_index = i\n",
    "                    break\n",
    "            sim_key_values = all_key_values[key_index]\n",
    "            if sim_key_values[0] != ori_value:\n",
    "                new_item['title'] = new_item['title'].replace(ori_value, sim_key_values[0], 1)\n",
    "                new_item['key_attr'][key] = sim_key_values[0]\n",
    "                change_num+=1\n",
    "    return new_item\n",
    "\n",
    "count = 0\n",
    "math_list = []\n",
    "not_math_list = []\n",
    "with open('../data/train_fine.txt', 'r', encoding='utf-8') as f:\n",
    "    lines = f.readlines()\n",
    "    new_dic = {}\n",
    "    for line in lines:\n",
    "        data = json.loads(line)\n",
    "        # math_list.append(json.dumps(data, ensure_ascii=False)+'\\n')\n",
    "        new_item = get_uni_pos_item(data, attr_key)\n",
    "        new_item is not None and math_list.append(json.dumps(new_item, ensure_ascii=False)+'\\n')\n",
    "        new_item = get_neg_item(new_item, attr_key)\n",
    "        new_item is not None and not_math_list.append(json.dumps(new_item, ensure_ascii=False)+'\\n')\n",
    "        # new_item = get_neg_item(data, attr_key)\n",
    "        # new_item is not None and not_math_list.append(json.dumps(new_item, ensure_ascii=False)+'\\n')\n",
    "        # count+=1\n",
    "        # if count >20:\n",
    "        #     break\n",
    "\n",
    "with open('../data/preprocessed_data/uni_pos_fine.txt', 'w', encoding='utf-8') as f:\n",
    "    f.writelines(math_list)\n",
    "with open('../data/preprocessed_data/uni_neg_fine.txt', 'w', encoding='utf-8') as f:\n",
    "    f.writelines(not_math_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'领型': [['高领', '半高领', '立领'],\n",
       "  ['连帽', '可脱卸帽'],\n",
       "  ['翻领', '衬衫领', 'POLO领', '方领', '娃娃领', '荷叶领'],\n",
       "  ['双层领'],\n",
       "  ['西装领'],\n",
       "  ['U型领'],\n",
       "  ['一字领'],\n",
       "  ['围巾领'],\n",
       "  ['堆堆领'],\n",
       "  ['V领'],\n",
       "  ['棒球领'],\n",
       "  ['圆领'],\n",
       "  ['斜领'],\n",
       "  ['亨利领']],\n",
       " '袖长': [['短袖', '五分袖'], ['九分袖', '长袖'], ['七分袖'], ['无袖']],\n",
       " '衣长': [['超短款', '短款', '常规款'], ['长款', '超长款'], ['中长款']],\n",
       " '版型': [['修身型', '标准型'], ['宽松型']],\n",
       " '裙长': [['短裙', '超短裙'], ['中裙', '中长裙'], ['长裙']],\n",
       " '穿着方式': [['套头'], ['开衫']],\n",
       " '类别': [['手提包'], ['单肩包'], ['斜挎包'], ['双肩包']],\n",
       " '裤型': [['O型裤', '锥形裤', '哈伦裤', '灯笼裤'],\n",
       "  ['铅笔裤', '直筒裤', '小脚裤'],\n",
       "  ['工装裤'],\n",
       "  ['紧身裤'],\n",
       "  ['背带裤'],\n",
       "  ['喇叭裤', '微喇裤'],\n",
       "  ['阔腿裤']],\n",
       " '裤长': [['短裤'], ['五分裤'], ['七分裤'], ['九分裤', '长裤']],\n",
       " '裤门襟': [['松紧'], ['拉链'], ['系带']],\n",
       " '闭合方式': [['松紧带'], ['拉链'], ['套筒', '套脚', '一脚蹬'], ['系带'], ['魔术贴'], ['搭扣']],\n",
       " '鞋帮高度': [['高帮', '中帮'], ['低帮']]}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('../data/preprocessed_data/attr_match.json', 'r', encoding='utf-8') as f:\n",
    "    attr_key = json.load(f)\n",
    "attr_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'领型': ['高领',\n",
       "  '连帽',\n",
       "  '翻领',\n",
       "  '双层领',\n",
       "  '西装领',\n",
       "  'U型领',\n",
       "  '一字领',\n",
       "  '围巾领',\n",
       "  '堆堆领',\n",
       "  'V领',\n",
       "  '棒球领',\n",
       "  '圆领',\n",
       "  '斜领',\n",
       "  '亨利领'],\n",
       " '袖长': ['短袖', '九分袖', '七分袖', '无袖'],\n",
       " '衣长': ['超短款', '长款', '中长款'],\n",
       " '版型': ['修身型', '宽松型'],\n",
       " '裙长': ['短裙', '中裙', '长裙'],\n",
       " '穿着方式': ['套头', '开衫'],\n",
       " '类别': ['手提包', '单肩包', '斜挎包', '双肩包'],\n",
       " '裤型': ['O型裤', '铅笔裤', '工装裤', '紧身裤', '背带裤', '喇叭裤', '阔腿裤'],\n",
       " '裤长': ['短裤', '五分裤', '七分裤', '九分裤'],\n",
       " '裤门襟': ['松紧', '拉链', '系带'],\n",
       " '闭合方式': ['松紧带', '拉链', '套筒', '系带', '魔术贴', '搭扣'],\n",
       " '鞋帮高度': ['高帮', '低帮']}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 统一字典\n",
    "uni_dict = {}\n",
    "for key, all_attr in attr_key.items():\n",
    "    attrs = []\n",
    "    for item_attr in all_attr:\n",
    "        attrs.append(item_attr[0])\n",
    "    uni_dict[key]=attrs\n",
    "with open('../data/preprocessed_data/uni_attr_match.json', 'w', encoding='utf-8') as f:\n",
    "    f.writelines([json.dumps(uni_dict, ensure_ascii=False)+'\\n'])\n",
    "uni_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'领型-高领': '高领',\n",
       " '领型-半高领': '高领',\n",
       " '领型-立领': '高领',\n",
       " '领型-连帽': '连帽',\n",
       " '领型-可脱卸帽': '连帽',\n",
       " '领型-翻领': '翻领',\n",
       " '领型-衬衫领': '翻领',\n",
       " '领型-POLO领': '翻领',\n",
       " '领型-方领': '翻领',\n",
       " '领型-娃娃领': '翻领',\n",
       " '领型-荷叶领': '翻领',\n",
       " '领型-双层领': '双层领',\n",
       " '领型-西装领': '西装领',\n",
       " '领型-U型领': 'U型领',\n",
       " '领型-一字领': '一字领',\n",
       " '领型-围巾领': '围巾领',\n",
       " '领型-堆堆领': '堆堆领',\n",
       " '领型-V领': 'V领',\n",
       " '领型-棒球领': '棒球领',\n",
       " '领型-圆领': '圆领',\n",
       " '领型-斜领': '斜领',\n",
       " '领型-亨利领': '亨利领',\n",
       " '袖长-短袖': '短袖',\n",
       " '袖长-五分袖': '短袖',\n",
       " '袖长-九分袖': '九分袖',\n",
       " '袖长-长袖': '九分袖',\n",
       " '袖长-七分袖': '七分袖',\n",
       " '袖长-无袖': '无袖',\n",
       " '衣长-超短款': '超短款',\n",
       " '衣长-短款': '超短款',\n",
       " '衣长-常规款': '超短款',\n",
       " '衣长-长款': '长款',\n",
       " '衣长-超长款': '长款',\n",
       " '衣长-中长款': '中长款',\n",
       " '版型-修身型': '修身型',\n",
       " '版型-标准型': '修身型',\n",
       " '版型-宽松型': '宽松型',\n",
       " '裙长-短裙': '短裙',\n",
       " '裙长-超短裙': '短裙',\n",
       " '裙长-中裙': '中裙',\n",
       " '裙长-中长裙': '中裙',\n",
       " '裙长-长裙': '长裙',\n",
       " '穿着方式-套头': '套头',\n",
       " '穿着方式-开衫': '开衫',\n",
       " '类别-手提包': '手提包',\n",
       " '类别-单肩包': '单肩包',\n",
       " '类别-斜挎包': '斜挎包',\n",
       " '类别-双肩包': '双肩包',\n",
       " '裤型-O型裤': 'O型裤',\n",
       " '裤型-锥形裤': 'O型裤',\n",
       " '裤型-哈伦裤': 'O型裤',\n",
       " '裤型-灯笼裤': 'O型裤',\n",
       " '裤型-铅笔裤': '铅笔裤',\n",
       " '裤型-直筒裤': '铅笔裤',\n",
       " '裤型-小脚裤': '铅笔裤',\n",
       " '裤型-工装裤': '工装裤',\n",
       " '裤型-紧身裤': '紧身裤',\n",
       " '裤型-背带裤': '背带裤',\n",
       " '裤型-喇叭裤': '喇叭裤',\n",
       " '裤型-微喇裤': '喇叭裤',\n",
       " '裤型-阔腿裤': '阔腿裤',\n",
       " '裤长-短裤': '短裤',\n",
       " '裤长-五分裤': '五分裤',\n",
       " '裤长-七分裤': '七分裤',\n",
       " '裤长-九分裤': '九分裤',\n",
       " '裤长-长裤': '九分裤',\n",
       " '裤门襟-松紧': '松紧',\n",
       " '裤门襟-拉链': '拉链',\n",
       " '裤门襟-系带': '系带',\n",
       " '闭合方式-松紧带': '松紧带',\n",
       " '闭合方式-拉链': '拉链',\n",
       " '闭合方式-套筒': '套筒',\n",
       " '闭合方式-套脚': '套筒',\n",
       " '闭合方式-一脚蹬': '套筒',\n",
       " '闭合方式-系带': '系带',\n",
       " '闭合方式-魔术贴': '魔术贴',\n",
       " '闭合方式-搭扣': '搭扣',\n",
       " '鞋帮高度-高帮': '高帮',\n",
       " '鞋帮高度-中帮': '高帮',\n",
       " '鞋帮高度-低帮': '低帮'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 统一字典\n",
    "dict_to_unit = {}\n",
    "for key, all_attr in attr_key.items():\n",
    "    for item_attr in all_attr:\n",
    "        for val in item_attr:\n",
    "            dict_to_unit[f'{key}-{val}'] = item_attr[0]\n",
    " \n",
    "with open('../data/preprocessed_data/dict_to_unit.json', 'w', encoding='utf-8') as f:\n",
    "    f.writelines([json.dumps(dict_to_unit, ensure_ascii=False)+'\\n'])\\\n",
    "\n",
    "dict_to_unit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'领型-高领': ['连帽',\n",
       "  '翻领',\n",
       "  '双层领',\n",
       "  '西装领',\n",
       "  'U型领',\n",
       "  '一字领',\n",
       "  '围巾领',\n",
       "  '堆堆领',\n",
       "  'V领',\n",
       "  '棒球领',\n",
       "  '圆领',\n",
       "  '斜领',\n",
       "  '亨利领'],\n",
       " '领型-连帽': ['高领',\n",
       "  '翻领',\n",
       "  '双层领',\n",
       "  '西装领',\n",
       "  'U型领',\n",
       "  '一字领',\n",
       "  '围巾领',\n",
       "  '堆堆领',\n",
       "  'V领',\n",
       "  '棒球领',\n",
       "  '圆领',\n",
       "  '斜领',\n",
       "  '亨利领'],\n",
       " '领型-翻领': ['高领',\n",
       "  '连帽',\n",
       "  '双层领',\n",
       "  '西装领',\n",
       "  'U型领',\n",
       "  '一字领',\n",
       "  '围巾领',\n",
       "  '堆堆领',\n",
       "  'V领',\n",
       "  '棒球领',\n",
       "  '圆领',\n",
       "  '斜领',\n",
       "  '亨利领'],\n",
       " '领型-双层领': ['高领',\n",
       "  '连帽',\n",
       "  '翻领',\n",
       "  '西装领',\n",
       "  'U型领',\n",
       "  '一字领',\n",
       "  '围巾领',\n",
       "  '堆堆领',\n",
       "  'V领',\n",
       "  '棒球领',\n",
       "  '圆领',\n",
       "  '斜领',\n",
       "  '亨利领'],\n",
       " '领型-西装领': ['高领',\n",
       "  '连帽',\n",
       "  '翻领',\n",
       "  '双层领',\n",
       "  'U型领',\n",
       "  '一字领',\n",
       "  '围巾领',\n",
       "  '堆堆领',\n",
       "  'V领',\n",
       "  '棒球领',\n",
       "  '圆领',\n",
       "  '斜领',\n",
       "  '亨利领'],\n",
       " '领型-U型领': ['高领',\n",
       "  '连帽',\n",
       "  '翻领',\n",
       "  '双层领',\n",
       "  '西装领',\n",
       "  '一字领',\n",
       "  '围巾领',\n",
       "  '堆堆领',\n",
       "  'V领',\n",
       "  '棒球领',\n",
       "  '圆领',\n",
       "  '斜领',\n",
       "  '亨利领'],\n",
       " '领型-一字领': ['高领',\n",
       "  '连帽',\n",
       "  '翻领',\n",
       "  '双层领',\n",
       "  '西装领',\n",
       "  'U型领',\n",
       "  '围巾领',\n",
       "  '堆堆领',\n",
       "  'V领',\n",
       "  '棒球领',\n",
       "  '圆领',\n",
       "  '斜领',\n",
       "  '亨利领'],\n",
       " '领型-围巾领': ['高领',\n",
       "  '连帽',\n",
       "  '翻领',\n",
       "  '双层领',\n",
       "  '西装领',\n",
       "  'U型领',\n",
       "  '一字领',\n",
       "  '堆堆领',\n",
       "  'V领',\n",
       "  '棒球领',\n",
       "  '圆领',\n",
       "  '斜领',\n",
       "  '亨利领'],\n",
       " '领型-堆堆领': ['高领',\n",
       "  '连帽',\n",
       "  '翻领',\n",
       "  '双层领',\n",
       "  '西装领',\n",
       "  'U型领',\n",
       "  '一字领',\n",
       "  '围巾领',\n",
       "  'V领',\n",
       "  '棒球领',\n",
       "  '圆领',\n",
       "  '斜领',\n",
       "  '亨利领'],\n",
       " '领型-V领': ['高领',\n",
       "  '连帽',\n",
       "  '翻领',\n",
       "  '双层领',\n",
       "  '西装领',\n",
       "  'U型领',\n",
       "  '一字领',\n",
       "  '围巾领',\n",
       "  '堆堆领',\n",
       "  '棒球领',\n",
       "  '圆领',\n",
       "  '斜领',\n",
       "  '亨利领'],\n",
       " '领型-棒球领': ['高领',\n",
       "  '连帽',\n",
       "  '翻领',\n",
       "  '双层领',\n",
       "  '西装领',\n",
       "  'U型领',\n",
       "  '一字领',\n",
       "  '围巾领',\n",
       "  '堆堆领',\n",
       "  'V领',\n",
       "  '圆领',\n",
       "  '斜领',\n",
       "  '亨利领'],\n",
       " '领型-圆领': ['高领',\n",
       "  '连帽',\n",
       "  '翻领',\n",
       "  '双层领',\n",
       "  '西装领',\n",
       "  'U型领',\n",
       "  '一字领',\n",
       "  '围巾领',\n",
       "  '堆堆领',\n",
       "  'V领',\n",
       "  '棒球领',\n",
       "  '斜领',\n",
       "  '亨利领'],\n",
       " '领型-斜领': ['高领',\n",
       "  '连帽',\n",
       "  '翻领',\n",
       "  '双层领',\n",
       "  '西装领',\n",
       "  'U型领',\n",
       "  '一字领',\n",
       "  '围巾领',\n",
       "  '堆堆领',\n",
       "  'V领',\n",
       "  '棒球领',\n",
       "  '圆领',\n",
       "  '亨利领'],\n",
       " '领型-亨利领': ['高领',\n",
       "  '连帽',\n",
       "  '翻领',\n",
       "  '双层领',\n",
       "  '西装领',\n",
       "  'U型领',\n",
       "  '一字领',\n",
       "  '围巾领',\n",
       "  '堆堆领',\n",
       "  'V领',\n",
       "  '棒球领',\n",
       "  '圆领',\n",
       "  '斜领'],\n",
       " '袖长-短袖': ['九分袖', '七分袖', '无袖'],\n",
       " '袖长-九分袖': ['短袖', '七分袖', '无袖'],\n",
       " '袖长-七分袖': ['短袖', '九分袖', '无袖'],\n",
       " '袖长-无袖': ['短袖', '九分袖', '七分袖'],\n",
       " '衣长-超短款': ['长款', '中长款'],\n",
       " '衣长-长款': ['超短款', '中长款'],\n",
       " '衣长-中长款': ['超短款', '长款'],\n",
       " '版型-修身型': ['宽松型'],\n",
       " '版型-宽松型': ['修身型'],\n",
       " '裙长-短裙': ['中裙', '长裙'],\n",
       " '裙长-中裙': ['短裙', '长裙'],\n",
       " '裙长-长裙': ['短裙', '中裙'],\n",
       " '穿着方式-套头': ['开衫'],\n",
       " '穿着方式-开衫': ['套头'],\n",
       " '类别-手提包': ['单肩包', '斜挎包', '双肩包'],\n",
       " '类别-单肩包': ['手提包', '斜挎包', '双肩包'],\n",
       " '类别-斜挎包': ['手提包', '单肩包', '双肩包'],\n",
       " '类别-双肩包': ['手提包', '单肩包', '斜挎包'],\n",
       " '裤型-O型裤': ['铅笔裤', '工装裤', '紧身裤', '背带裤', '喇叭裤', '阔腿裤'],\n",
       " '裤型-铅笔裤': ['O型裤', '工装裤', '紧身裤', '背带裤', '喇叭裤', '阔腿裤'],\n",
       " '裤型-工装裤': ['O型裤', '铅笔裤', '紧身裤', '背带裤', '喇叭裤', '阔腿裤'],\n",
       " '裤型-紧身裤': ['O型裤', '铅笔裤', '工装裤', '背带裤', '喇叭裤', '阔腿裤'],\n",
       " '裤型-背带裤': ['O型裤', '铅笔裤', '工装裤', '紧身裤', '喇叭裤', '阔腿裤'],\n",
       " '裤型-喇叭裤': ['O型裤', '铅笔裤', '工装裤', '紧身裤', '背带裤', '阔腿裤'],\n",
       " '裤型-阔腿裤': ['O型裤', '铅笔裤', '工装裤', '紧身裤', '背带裤', '喇叭裤'],\n",
       " '裤长-短裤': ['五分裤', '七分裤', '九分裤'],\n",
       " '裤长-五分裤': ['短裤', '七分裤', '九分裤'],\n",
       " '裤长-七分裤': ['短裤', '五分裤', '九分裤'],\n",
       " '裤长-九分裤': ['短裤', '五分裤', '七分裤'],\n",
       " '裤门襟-松紧': ['拉链', '系带'],\n",
       " '裤门襟-拉链': ['松紧', '系带'],\n",
       " '裤门襟-系带': ['松紧', '拉链'],\n",
       " '闭合方式-松紧带': ['拉链', '套筒', '系带', '魔术贴', '搭扣'],\n",
       " '闭合方式-拉链': ['松紧带', '套筒', '系带', '魔术贴', '搭扣'],\n",
       " '闭合方式-套筒': ['松紧带', '拉链', '系带', '魔术贴', '搭扣'],\n",
       " '闭合方式-系带': ['松紧带', '拉链', '套筒', '魔术贴', '搭扣'],\n",
       " '闭合方式-魔术贴': ['松紧带', '拉链', '套筒', '系带', '搭扣'],\n",
       " '闭合方式-搭扣': ['松紧带', '拉链', '套筒', '系带', '魔术贴'],\n",
       " '鞋帮高度-高帮': ['低帮'],\n",
       " '鞋帮高度-低帮': ['高帮']}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 负例采样字典\n",
    "unit_neg_dict = {}\n",
    "for key, all_attr in uni_dict.items():\n",
    "    for item_attr in all_attr:\n",
    "        new_all_attr = all_attr.copy()\n",
    "        new_all_attr.remove(item_attr)\n",
    "        unit_neg_dict[f'{key}-{item_attr}'] = new_all_attr\n",
    "with open('../data/preprocessed_data/unit_neg_sample_dict.json', 'w', encoding='utf-8') as f:\n",
    "    f.writelines([json.dumps(unit_neg_dict, ensure_ascii=False)+'\\n'])\n",
    "\n",
    "unit_neg_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "50000it [01:55, 431.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "# remove years [fine]\n",
    "fine_data = '../data/train_fine.txt'\n",
    "new_fine_data = '../data/preprocessed_data/unit_train_fine.txt'\n",
    "\n",
    "rets = []\n",
    "i = 0\n",
    "with open(fine_data, 'r') as f:\n",
    "    for i, data in enumerate(tqdm(f)):\n",
    "        data = json.loads(data)\n",
    "        data['title'] = re.sub(r'\\d{1,4}年','', data['title'])\n",
    "\n",
    "        # unit attr\n",
    "        keys = [x for x in data['match'].keys() if x != '图文']\n",
    "        for key in keys:\n",
    "            ori_value = data['key_attr'][key]\n",
    "            new_value = dict_to_unit[f'{key}-{ori_value}']\n",
    "            data['title'] = data['title'].replace(ori_value, new_value, 1)\n",
    "            data['key_attr'][key] = new_value\n",
    "        rets.append(json.dumps(data, ensure_ascii=False)+'\\n')\n",
    "        \n",
    "with open(new_fine_data, 'w') as f:\n",
    "    f.writelines(rets)\n",
    "print(len(rets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(new_fine_data, 'w') as f:\n",
    "    f.writelines(rets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm \n",
    "import itertools\n",
    "def load_attr_dict(file):\n",
    "    # 读取属性字典\n",
    "    with open(file, 'r') as f:\n",
    "        attr_dict = {}\n",
    "        for attr, attrval_list in json.load(f).items():\n",
    "            attrval_list = list(map(lambda x: x.split('='), attrval_list))\n",
    "            attr_dict[attr] = list(itertools.chain.from_iterable(attrval_list))\n",
    "    return attr_dict\n",
    "\n",
    "def match_attrval(title, attr, attr_dict):\n",
    "    # 在title中匹配属性值\n",
    "    attrvals = \"|\".join(attr_dict[attr])\n",
    "    ret = re.findall(attrvals, title)\n",
    "    # return \"{}{}\".format(attr, ''.join(ret))\n",
    "    return \"{}\".format(''.join(ret))  \n",
    "\n",
    "# load attribute dict\n",
    "attr_dict_file = \"../data/attr_to_attrvals.json\"\n",
    "attr_dict = load_attr_dict(attr_dict_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100000it [02:49, 591.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "89588 10412\n"
     ]
    }
   ],
   "source": [
    "# remove years and get attributes [coarse]\n",
    "# 裤门襟和鞋带的属性有重合，所以需要额外的判断机制\n",
    "coarse_data = '../data/train_coarse.txt'\n",
    "new_coarse_data = '../data/preprocessed_data/unit_coarse_fine.txt'\n",
    "new_neg_coarse_data = '../data/preprocessed_data/unit_neg_coarse_fine.txt'\n",
    "\n",
    "coarse_rets = []\n",
    "neg_coarse_rets = []\n",
    "querys = attr_dict.keys()\n",
    "i = 0\n",
    "with open(coarse_data, 'r') as f:\n",
    "    for i, data in enumerate(tqdm(f)):\n",
    "        data = json.loads(data)\n",
    "        data['title'] = re.sub(r'\\d{1,4}年','', data['title'])\n",
    "        \n",
    "        if data['match']['图文'] == 1:\n",
    "            for query in querys:\n",
    "                values = attr_dict[query]\n",
    "                if (query == '裤门襟') and ('裤' not in data['title'] ):\n",
    "                    continue\n",
    "                if (query == '闭合方式') and ('裤' in data['title'] ):\n",
    "                    continue\n",
    "                for value in values:\n",
    "                    if value in data['title']:\n",
    "                        data['key_attr'][query] = value\n",
    "                        data['match'][query] = 1\n",
    "            # unit attr\n",
    "            keys = [x for x in data['match'].keys() if x != '图文']\n",
    "            for key in keys:\n",
    "                ori_value = data['key_attr'][key]\n",
    "                new_value = dict_to_unit[f'{key}-{ori_value}']\n",
    "                data['title'] = data['title'].replace(ori_value, new_value, 1)\n",
    "                data['key_attr'][key] = new_value\n",
    "            coarse_rets.append(json.dumps(data, ensure_ascii=False)+'\\n') \n",
    "        else:\n",
    "            for query in querys:\n",
    "                values = attr_dict[query]\n",
    "                if (query == '裤门襟') and ('裤' not in data['title'] ):\n",
    "                    continue\n",
    "                if (query == '闭合方式') and ('裤' in data['title'] ):\n",
    "                    continue\n",
    "                for value in values:\n",
    "                    if value in data['title']:\n",
    "                        data['key_attr'][query] = value\n",
    "                        data['match'][query] = -1\n",
    "            keys = [x for x in data['match'].keys() if x != '图文']\n",
    "            for key in keys:\n",
    "                ori_value = data['key_attr'][key]\n",
    "                new_value = dict_to_unit[f'{key}-{ori_value}']\n",
    "                data['title'] = data['title'].replace(ori_value, new_value, 1)\n",
    "                data['key_attr'][key] = new_value\n",
    "            neg_coarse_rets.append(json.dumps(data, ensure_ascii=False)+'\\n') \n",
    "\n",
    "with open(new_coarse_data, 'w') as f:\n",
    "    f.writelines(coarse_rets)\n",
    "\n",
    "with open(new_neg_coarse_data, 'w') as f:\n",
    "    f.writelines(neg_coarse_rets)\n",
    "    \n",
    "print(len(coarse_rets), len(neg_coarse_rets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_len = []\n",
    "\n",
    "data1 = '../data/preprocessed_data/unit_train_fine.txt'\n",
    "data2 = '../data/preprocessed_data/unit_coarse_fine89588.txt'\n",
    "data3 = '../data/preprocessed_data/unit_neg_coarse_fine10412.txt'\n",
    "\n",
    "\n",
    "with open(coarse_data, 'r') as f:\n",
    "    for i, data in enumerate(tqdm(f)):\n",
    "        data = json.loads(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ab7a9148291449fc4391f360f086843f6cf18b6c2df8af6702dacf91b204e632"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
