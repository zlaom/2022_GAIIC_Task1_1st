{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ltp import LTP\n",
    "from tqdm import tqdm\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ltp = LTP(path='base') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 取得属性字典并添加到词表\n",
    "def get_dict(file):\n",
    "    with open(file, 'r') as f:\n",
    "        all_attr = []\n",
    "        for attr, attrval_list in json.load(f).items():\n",
    "            for x in attrval_list:\n",
    "                x = x.split('=')\n",
    "                for k in x:\n",
    "                    all_attr.append(k)\n",
    "    return all_attr\n",
    "attr_dict_file = \"data/original_data/attr_to_attrvals.json\"\n",
    "all_attr = get_dict(attr_dict_file)\n",
    "extra_words = []\n",
    "extra_words.append(['牛津布', '仿皮', '吸湿', '吸汗', '防滑', '抗冲击', '微弹', '加绒'])\n",
    "extra_words.append(['上青', '上青色', '上青绿', '羊绒衫'])\n",
    "extra_words.append(['休闲鞋', '工装鞋', '男包', '女包', '运动裤', '休闲裤', '加厚领'])\n",
    "extra_words.append(['加厚', '薄款', '厚款', '短款', '短外套'])\n",
    "extra_words.append(['不加绒', '无扣', '无弹力', '无弹', '无拉链'])\n",
    "extra_words.append(['一粒扣', '两粒扣', '暗扣', '三粒扣', '系扣'])\n",
    "extra_words.append(['大红色', '大花'])\n",
    "for extra in extra_words:   \n",
    "    all_attr = all_attr + extra\n",
    "\n",
    "ltp.init_dict(path=\"user_dict.txt\", max_window=6)\n",
    "ltp.add_words(words=all_attr, max_window=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 统计训练集的词表和词频\n",
    "fine_file = 'data/train/fine45000.txt'\n",
    "word_dict = {}\n",
    "with open(fine_file, 'r') as f:\n",
    "    for line in tqdm(f):\n",
    "        item = json.loads(line)\n",
    "        segment, _ = ltp.seg([item['title']])\n",
    "        for word in segment[0]:\n",
    "            word = word.upper() # 字母统一为大写\n",
    "            if word in word_dict:\n",
    "                word_dict[word] += 1\n",
    "            else:\n",
    "                word_dict[word] = 1\n",
    "\n",
    "coarse_file = 'data/train/coarse89588.txt'\n",
    "with open(coarse_file, 'r') as f:\n",
    "    for line in tqdm(f):\n",
    "        item = json.loads(line)\n",
    "        segment, _ = ltp.seg([item['title']])\n",
    "        for word in segment[0]:\n",
    "            word = word.upper() # 字母统一为大写\n",
    "            if word in word_dict:\n",
    "                word_dict[word] += 1\n",
    "            else:\n",
    "                word_dict[word] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 为了不丢失花大量时间生成的词表先做一个拷贝\n",
    "import copy \n",
    "copy_dict = copy.deepcopy(word_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 备用代码\n",
    "# word_dict = copy.deepcopy(copy_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 统一大量颜色的别称\n",
    "color_list = ['兰','蓝','灰','绿','粉','红','黄','青','紫','白','黑','骆','橙','杏','咖','棕','啡','褐','银','金','橘','藏']\n",
    "keys = []\n",
    "for key in  word_dict.keys():\n",
    "    keys.append(key)\n",
    "for key in keys:\n",
    "    for i in key:\n",
    "        if i in color_list and word_dict[key] < 50:\n",
    "            if i in word_dict:\n",
    "                word_dict[i] += word_dict[key]\n",
    "            else:\n",
    "                word_dict[i] = word_dict[key]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 删除出现次数少的词\n",
    "keys = []\n",
    "for key in word_dict.keys():\n",
    "    keys.append(key)\n",
    "for key in keys:\n",
    "    if word_dict[key] < 50:\n",
    "        del word_dict[key]\n",
    "    if key == '/':\n",
    "        del word_dict[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存词表\n",
    "with open('word_dict.json', 'w') as f:\n",
    "    json.dump(word_dict, f, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存处理后的词表\n",
    "with open('processed_word_dict.json', 'w') as f:\n",
    "    json.dump(word_dict, f, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读入保存的词表\n",
    "with open('processed_word_dict.json', 'r') as f:\n",
    "    processed_word_dict = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 生成分好的title词集\n",
    "fine_file = 'data/train/fine45000.txt'\n",
    "save_fine_file = 'data/train/fine45000_OnlySplit.txt'\n",
    "rets = []\n",
    "i = 0\n",
    "with open(fine_file, 'r') as f:\n",
    "    for line in tqdm(f):\n",
    "        new_item = {}\n",
    "        item = json.loads(line)\n",
    "        segment, _ = ltp.seg([item['title']])\n",
    "        for word in segment[0]:\n",
    "            word = word.upper() # 字母统一为大写\n",
    "        new_item['title_split'] = segment[0]\n",
    "        # 更改保存的顺序，便于查看\n",
    "        rets.append(json.dumps(new_item, ensure_ascii=False)+'\\n')\n",
    "        \n",
    "        # if i>20000:\n",
    "        #     break\n",
    "        # i += 1\n",
    "        \n",
    "with open(save_fine_file, 'w') as f:\n",
    "    f.writelines(rets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 生成分好的title词集\n",
    "fine_file = 'data/train/fine45000.txt'\n",
    "save_fine_file = 'data/train/fine45000_Sample.txt'\n",
    "rets = []\n",
    "i = 0\n",
    "with open(fine_file, 'r') as f:\n",
    "    for line in tqdm(f):\n",
    "        item = json.loads(line)\n",
    "        segment, _ = ltp.seg([item['title']])\n",
    "        for word in segment[0]:\n",
    "            word = word.upper() # 字母统一为大写\n",
    "        item['title_split'] = segment[0]\n",
    "        # 更改保存的顺序，便于查看\n",
    "        feature = item['feature']\n",
    "        del item['feature']\n",
    "        item['feature'] = feature\n",
    "        rets.append(json.dumps(item, ensure_ascii=False)+'\\n')\n",
    "        \n",
    "        if i>500:\n",
    "            break\n",
    "        i += 1\n",
    "        \n",
    "with open(save_fine_file, 'w') as f:\n",
    "    f.writelines(rets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coarse_file = 'data/train/coarse89588.txt'\n",
    "save_coarse_file = 'data/train/coarse89588_S.txt'\n",
    "rets = []\n",
    "i = 0\n",
    "with open(coarse_file, 'r') as f:\n",
    "    for line in tqdm(f):\n",
    "        item = json.loads(line)\n",
    "        segment, _ = ltp.seg([item['title']])\n",
    "        for word in segment[0]:\n",
    "            word = word.upper() # 字母统一为大写\n",
    "        item['title_split'] = segment[0]\n",
    "        # 更改保存的顺序，便于查看\n",
    "        feature = item['feature']\n",
    "        del item['feature']\n",
    "        item['feature'] = feature\n",
    "        rets.append(json.dumps(item, ensure_ascii=False)+'\\n')\n",
    "        \n",
    "        # if i>500:\n",
    "        #     break\n",
    "        # i += 1\n",
    "\n",
    "with open(save_coarse_file, 'w') as f:\n",
    "    f.writelines(rets)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
