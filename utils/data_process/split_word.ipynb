{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ltp import LTP\n",
    "from tqdm import tqdm\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ltp = LTP(path='pretrained_model/ltp_base') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 取得属性字典并添加到词表\n",
    "def get_dict(file):\n",
    "    with open(file, 'r') as f:\n",
    "        all_attr = []\n",
    "        for attr, attrval_list in json.load(f).items():\n",
    "            for x in attrval_list:\n",
    "                x = x.split('=')\n",
    "                for k in x:\n",
    "                    all_attr.append(k)\n",
    "    return all_attr\n",
    "attr_dict_file = \"original_data/attr_to_attrvals.json\"\n",
    "all_attr = get_dict(attr_dict_file)\n",
    "extra_words = []\n",
    "extra_words.append(['牛津布', '仿皮', '吸湿', '吸汗', '防滑', '抗冲击', '微弹', '加绒'])\n",
    "extra_words.append(['上青', '上青色', '上青绿', '羊绒衫'])\n",
    "extra_words.append(['休闲鞋', '工装鞋', '男包', '女包', '运动裤', '休闲裤', '加厚领'])\n",
    "extra_words.append(['加厚', '薄款', '厚款', '短款', '短外套'])\n",
    "extra_words.append(['不加绒', '无扣', '无弹力', '无弹', '无拉链'])\n",
    "extra_words.append(['一粒扣', '两粒扣', '暗扣', '三粒扣', '系扣'])\n",
    "extra_words.append(['大红色', '大花'])\n",
    "for extra in extra_words:   \n",
    "    all_attr = all_attr + extra\n",
    "\n",
    "ltp.init_dict(path=\"user_dict.txt\", max_window=6)\n",
    "ltp.add_words(words=all_attr, max_window=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "45000it [08:44, 85.79it/s]\n",
      "89588it [17:24, 85.81it/s]\n"
     ]
    }
   ],
   "source": [
    "# 统计训练集的词表和词频\n",
    "fine_file = 'train/fine45000.txt'\n",
    "word_dict = {}\n",
    "with open(fine_file, 'r') as f:\n",
    "    for line in tqdm(f):\n",
    "        item = json.loads(line)\n",
    "        segment, _ = ltp.seg([item['title']])\n",
    "        for word in segment[0]:\n",
    "            word = word.upper() # 字母统一为大写\n",
    "            if word in word_dict:\n",
    "                word_dict[word] += 1\n",
    "            else:\n",
    "                word_dict[word] = 1\n",
    "\n",
    "coarse_file = 'train/coarse89588.txt'\n",
    "with open(coarse_file, 'r') as f:\n",
    "    for line in tqdm(f):\n",
    "        item = json.loads(line)\n",
    "        segment, _ = ltp.seg([item['title']])\n",
    "        for word in segment[0]:\n",
    "            word = word.upper() # 字母统一为大写\n",
    "            if word in word_dict:\n",
    "                word_dict[word] += 1\n",
    "            else:\n",
    "                word_dict[word] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 为了不丢失花大量时间生成的词表先做一个拷贝\n",
    "import copy \n",
    "copy_dict = copy.deepcopy(word_dict)\n",
    "\n",
    "# 保存词表\n",
    "with open('split_word/base_word_dict/word_dict.json', 'w') as f:\n",
    "    json.dump(word_dict, f, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 备用代码\n",
    "# word_dict = copy.deepcopy(copy_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 统一大量颜色的别称\n",
    "color_list = ['兰','蓝','灰','绿','粉','红','黄','青','紫','白','黑','骆','橙','杏','咖','棕','啡','褐','银','金','橘','藏']\n",
    "keys = []\n",
    "for key in  word_dict.keys():\n",
    "    keys.append(key)\n",
    "for key in keys:\n",
    "    for i in key:\n",
    "        if i in color_list and word_dict[key] < 50:\n",
    "            if i in word_dict:\n",
    "                word_dict[i] += word_dict[key]\n",
    "            else:\n",
    "                word_dict[i] = word_dict[key]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 删除出现次数少的词\n",
    "keys = []\n",
    "for key in word_dict.keys():\n",
    "    keys.append(key)\n",
    "for key in keys:\n",
    "    if word_dict[key] < 50:\n",
    "        del word_dict[key]\n",
    "    if key == '/':\n",
    "        del word_dict[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 去掉没有单独意义的字词\n",
    "delete_words = ['色','小','本','中','新','款','加','底','件','不']\n",
    "for word in delete_words:\n",
    "    del word_dict[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存处理后的词表\n",
    "with open('split_word/base_word_dict/processed_word_dict.json', 'w') as f:\n",
    "    json.dump(word_dict, f, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读入保存的词表\n",
    "with open('processed_word_dict.json', 'r') as f:\n",
    "    processed_word_dict = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "45000it [09:30, 78.90it/s]\n"
     ]
    }
   ],
   "source": [
    "# 生成分好的title词集\n",
    "fine_file = 'train/fine45000.txt'\n",
    "save_fine_file = 'split_word/fine45000.txt'\n",
    "\n",
    "vocab = list(word_dict.keys())\n",
    "color_list = ['兰','蓝','灰','绿','粉','红','黄','青','紫','白','黑','骆','橙','杏','咖','棕','啡','褐','银','金','橘','藏']\n",
    "rets = []\n",
    "i = 0\n",
    "with open(fine_file, 'r') as f:\n",
    "    for line in tqdm(f):\n",
    "        item = json.loads(line)\n",
    "        segment, _ = ltp.seg([item['title']])\n",
    "        item['title_split'] = segment[0]\n",
    "        vocab_split = []\n",
    "        for word in segment[0]:\n",
    "            word = word.upper() # 字母统一为大写\n",
    "            if word in vocab:\n",
    "                vocab_split.append(word)\n",
    "            else: # 颜色提取\n",
    "                for c in word:\n",
    "                    if c in color_list:\n",
    "                        vocab_split.append(c)\n",
    "        item['vocab_split'] = vocab_split\n",
    "        if not vocab_split:\n",
    "            print(item['title'])\n",
    "        # 更改保存的顺序，便于查看\n",
    "        feature = item['feature']\n",
    "        del item['feature']\n",
    "        item['feature'] = feature\n",
    "        rets.append(json.dumps(item, ensure_ascii=False)+'\\n')\n",
    "        \n",
    "        # if i>500:\n",
    "        #     break\n",
    "        # i += 1\n",
    "        \n",
    "with open(save_fine_file, 'w') as f:\n",
    "    f.writelines(rets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "89588it [19:00, 78.56it/s]\n"
     ]
    }
   ],
   "source": [
    "coarse_file = 'train/coarse89588.txt'\n",
    "save_coarse_file = 'split_word/coarse89588.txt'\n",
    "\n",
    "vocab = list(word_dict.keys())\n",
    "color_list = ['兰','蓝','灰','绿','粉','红','黄','青','紫','白','黑','骆','橙','杏','咖','棕','啡','褐','银','金','橘','藏']\n",
    "rets = []\n",
    "i = 0\n",
    "with open(coarse_file, 'r') as f:\n",
    "    for line in tqdm(f):\n",
    "        item = json.loads(line)\n",
    "        segment, _ = ltp.seg([item['title']])\n",
    "        item['title_split'] = segment[0]\n",
    "        vocab_split = []\n",
    "        for word in segment[0]:\n",
    "            word = word.upper() # 字母统一为大写\n",
    "            if word in vocab:\n",
    "                vocab_split.append(word)\n",
    "            else: # 颜色提取\n",
    "                for c in word:\n",
    "                    if c in color_list:\n",
    "                        vocab_split.append(c)\n",
    "        item['vocab_split'] = vocab_split\n",
    "        if not vocab_split:\n",
    "            print(item['title'])\n",
    "        # 更改保存的顺序，便于查看\n",
    "        feature = item['feature']\n",
    "        del item['feature']\n",
    "        item['feature'] = feature\n",
    "        rets.append(json.dumps(item, ensure_ascii=False)+'\\n')\n",
    "        \n",
    "        # if i>500:\n",
    "        #     break\n",
    "        # i += 1\n",
    "        \n",
    "with open(save_coarse_file, 'w') as f:\n",
    "    f.writelines(rets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a7cf278074aea429de13d88b394304564a7b018f3e18e13089daf4fad90abe0d"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
