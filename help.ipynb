{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch \n",
    "import json\n",
    "import numpy as np \n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm \n",
    "\n",
    "from model.bert.bertconfig import BertConfig\n",
    "from model.fusemodel import DesignFuseModel, FuseModel2TasksNewDropout\n",
    "\n",
    "image_dropout = 0.3\n",
    "vocab_dict_file = 'data/new_data/vocab/vocab_dict.json'\n",
    "vocab_file = 'data/new_data/vocab/vocab.txt'\n",
    "attr_dict_file = 'data/new_data/equal_processed_data/dict/attr_relation_dict.json'\n",
    "\n",
    "split_layers = 0\n",
    "fuse_layers = 3\n",
    "n_img_expand = 6\n",
    "\n",
    "# model\n",
    "split_config = BertConfig(num_hidden_layers=split_layers)\n",
    "fuse_config = BertConfig(num_hidden_layers=fuse_layers, image_dropout=image_dropout)\n",
    "model = DesignFuseModel(split_config, fuse_config, vocab_file, n_img_expand=n_img_expand, word_match=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31080194"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(param.numel() for param in model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "157031430"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "52343810 * 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 62.00 MiB (GPU 0; 23.70 GiB total capacity; 534.54 MiB already allocated; 46.56 MiB free; 568.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mltp\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LTP\n\u001b[0;32m----> 2\u001b[0m ltp \u001b[38;5;241m=\u001b[39m \u001b[43mLTP\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdata/pretrained_model/ltp_base/\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/ltp/frontend.py:147\u001b[0m, in \u001b[0;36mLTP.__init__\u001b[0;34m(self, path, device, **kwargs)\u001b[0m\n\u001b[1;32m    144\u001b[0m parser \u001b[38;5;241m=\u001b[39m Model\u001b[38;5;241m.\u001b[39madd_model_specific_args(parser)\n\u001b[1;32m    145\u001b[0m model_args \u001b[38;5;241m=\u001b[39m parser\u001b[38;5;241m.\u001b[39mparse_args(args\u001b[38;5;241m=\u001b[39m[], namespace\u001b[38;5;241m=\u001b[39mckpt[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_config\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m--> 147\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[43mModel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    148\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mload_state_dict(ckpt[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m'\u001b[39m], strict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39meval()\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/ltp/nn/module.py:44\u001b[0m, in \u001b[0;36mBaseModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     42\u001b[0m out \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_nn\u001b[38;5;241m.\u001b[39m_parse_to(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__update_properties(device\u001b[38;5;241m=\u001b[39mout[\u001b[38;5;241m0\u001b[39m], dtype\u001b[38;5;241m=\u001b[39mout[\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m---> 44\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:907\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    903\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    904\u001b[0m                     non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[1;32m    905\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, non_blocking)\n\u001b[0;32m--> 907\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:578\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    576\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_apply\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn):\n\u001b[1;32m    577\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 578\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    580\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    581\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    582\u001b[0m             \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    583\u001b[0m             \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    588\u001b[0m             \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    589\u001b[0m             \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:578\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    576\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_apply\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn):\n\u001b[1;32m    577\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 578\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    580\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    581\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    582\u001b[0m             \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    583\u001b[0m             \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    588\u001b[0m             \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    589\u001b[0m             \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:578\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    576\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_apply\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn):\n\u001b[1;32m    577\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 578\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    580\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    581\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    582\u001b[0m             \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    583\u001b[0m             \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    588\u001b[0m             \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    589\u001b[0m             \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:601\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    597\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    598\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    599\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    600\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 601\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    602\u001b[0m should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    603\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m should_use_set_data:\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:905\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    902\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m    903\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    904\u001b[0m                 non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[0;32m--> 905\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 62.00 MiB (GPU 0; 23.70 GiB total capacity; 534.54 MiB already allocated; 46.56 MiB free; 568.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "from ltp import LTP\n",
    "ltp = LTP('data/pretrained_model/ltp_base/') # 默认加载 Small 模型\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "137875216"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(param.numel() for param in ltp.model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "362124784"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "500000000 - 137875216"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.863 GB'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "size = 500000000*32/8/1024/1024/1024\n",
    "f\"{size:.3f} GB\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "157029123"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "title_model = 52343041\n",
    "title_model*3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100409579"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "362124784-title_model*5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13085760.25"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "52343041/4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = [1,2,3]\n",
    "for i in l:\n",
    "    if i == 2:\n",
    "        del i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 4.])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.tensor(1.)\n",
    "b = torch.tensor(4.)\n",
    "torch.tensor([a, b])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from tqdm import tqdm \n",
    "val_file = 'data/new_data/divided/title/shuffle/fine5000_0.25posaug.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10848it [00:00, 31086.15it/s]\n"
     ]
    }
   ],
   "source": [
    "with open(val_file, 'r') as f:\n",
    "    for line in tqdm(f):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import torch \n",
    "import json\n",
    "import numpy as np \n",
    "from tqdm import tqdm \n",
    "import copy \n",
    "\n",
    "seed = 0\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "\n",
    "val_file = 'data/new_data/divided/attr/fine5000.txt'\n",
    "vocab_dict_file = 'data/new_data/vocab/vocab_dict.json'\n",
    "vocab_file = 'data/new_data/vocab/vocab.txt'\n",
    "relation_dict_file = 'data/new_data/equal_processed_data/attr_relation_dict.json'\n",
    "save_file = 'data/new_data/divided/attr/val/fine5000.txt'\n",
    "with open(relation_dict_file, 'r') as f:\n",
    "    relation_dict = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5000it [00:18, 277.28it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "rets = []\n",
    "with open(val_file, 'r') as f:\n",
    "    for line in tqdm(f):\n",
    "        item = json.loads(line)\n",
    "        if item['key_attr']: # 必须有属性\n",
    "            for query, attr in item['key_attr'].items():\n",
    "                key_attr = {}\n",
    "                match = {}\n",
    "                new_item = copy.deepcopy(item)\n",
    "                if random.random() < 0.5: # 替换，随机挑选一个词替换\n",
    "                    label = 0\n",
    "                    attr_list = random.sample(relation_dict[attr]['similar_attr'], 1)[0]\n",
    "                    if len(attr_list) == 1:\n",
    "                        split = attr_list\n",
    "                    else:\n",
    "                        attr = random.sample(attr_list, 1)[0]\n",
    "                        split = [attr]\n",
    "                else: \n",
    "                    label = 1\n",
    "                    split = [attr]\n",
    "                    \n",
    "                key_attr[query] = split[0]\n",
    "                match[query] = label\n",
    "                new_item['key_attr'] = key_attr\n",
    "                new_item['match'] = match\n",
    "                rets.append(json.dumps(new_item, ensure_ascii=False)+'\\n')\n",
    "        \n",
    "with open(save_file, 'w') as f:\n",
    "    f.writelines(rets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.12132"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = 0.121324334\n",
    "round(a, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor([[1.,2],[3,4]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2689, 0.7311],\n",
       "        [0.2689, 0.7311]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.softmax(a, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = np.array([[1,2],[3],[4]], dtype='object')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = [[1,2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 2]]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.sample(l, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "a must be 1-dimensional",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/haoquan/GAIIC2022/help.ipynb Cell 3'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B10.16.16.232/home/haoquan/GAIIC2022/help.ipynb#ch0000041vscode-remote?line=0'>1</a>\u001b[0m np\u001b[39m.\u001b[39;49mrandom\u001b[39m.\u001b[39;49mchoice(l)\n",
      "File \u001b[0;32mmtrand.pyx:911\u001b[0m, in \u001b[0;36mnumpy.random.mtrand.RandomState.choice\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: a must be 1-dimensional"
     ]
    }
   ],
   "source": [
    "np.random.choice(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[429, 617, 382]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = 0.5\n",
    "num_list = [205+83,317+723,100]\n",
    "\n",
    "N = len(num_list)\n",
    "S = sum(num_list)\n",
    "copy_list = num_list.copy()\n",
    "for i in range(N):\n",
    "    n = copy_list[i]\n",
    "    pos = n * (1-p)\n",
    "    neg = (S-n) * p / (N-1)\n",
    "    num_list[i] = int(pos + neg)\n",
    "num_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[108, 187, 131]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[40+68,43+144,131]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "attr_dict_file = 'data/new_data/equal_processed_data/attr_to_attrvals.json'\n",
    "vocab_dict_file = 'data/new_data/vocab/vocab_dict.json'\n",
    "with open(attr_dict_file, 'r') as f:\n",
    "    attr_dict = json.load(f)\n",
    "with open(vocab_dict_file, 'r') as f:\n",
    "    vocab_dict = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "total = 0\n",
    "num_attr = 0\n",
    "for query, attr_list in attr_dict.items():\n",
    "    for attr in attr_list:\n",
    "        num_attr += vocab_dict[attr]\n",
    "for word, n in vocab_dict.items():\n",
    "    total += n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "302501"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_attr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "995299"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "\n",
    "attr_dict_file = 'data/new_data/equal_processed_data/attr_to_attrvals.json'\n",
    "vocab_dict_file = 'data/new_data/vocab/vocab_dict.json'\n",
    "with open(attr_dict_file, 'r') as f:\n",
    "    attr_dict = json.load(f)\n",
    "with open(vocab_dict_file, 'r') as f:\n",
    "    vocab_dict = json.load(f)\n",
    "def get_negative_dict(attr_dict, vocab_dict):\n",
    "    proba_negative_dict = {}\n",
    "    for query, attr_list in attr_dict.items():\n",
    "        proba_negative_dict[query] = {}\n",
    "        proba_negative_dict[query]['attr_list'] = attr_list\n",
    "        proba_list = []\n",
    "        for attr in attr_list:\n",
    "            proba_list.append(vocab_dict[attr])\n",
    "        proba_negative_dict[query]['attr_freq'] = proba_list\n",
    "            \n",
    "    return proba_negative_dict\n",
    "\n",
    "proba_negative_dict = get_negative_dict(attr_dict, vocab_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "attr_save_file = 'data/new_data/equal_processed_data/attr_analysis.json'\n",
    "with open(attr_save_file, 'w') as f:\n",
    "    json.dump(proba_negative_dict, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100000it [01:52, 889.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "89588\n",
      "10412\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from tqdm import tqdm \n",
    "import json\n",
    "fine_path = 'data/new_data/split_word/coarse10412.txt'\n",
    "fine_val_path = 'data/new_data/split_word/coarse10412_2.txt'\n",
    "train_rets = []\n",
    "val_rets = []\n",
    "\n",
    "with open(fine_path, 'r') as f:\n",
    "    for i, data in enumerate(tqdm(f)):\n",
    "        data = json.loads(data)\n",
    "        if len(train_rets) < 89588:      \n",
    "            train_rets.append(json.dumps(data, ensure_ascii=False)+'\\n')\n",
    "        else:\n",
    "            val_rets.append(json.dumps(data, ensure_ascii=False)+'\\n')\n",
    "print(len(train_rets))\n",
    "print(len(val_rets))\n",
    "\n",
    "# with open(fine_train_path, 'w') as f:\n",
    "#     f.writelines(train_rets)\n",
    "with open(fine_val_path, 'w') as f:\n",
    "    f.writelines(val_rets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = 'data/equal_processed_data/nofeat/test10000_analysis.txt'\n",
    "\n",
    "\n",
    "proba_negative_dict = {}\n",
    "for query, attr_list in attr_dict.items():\n",
    "    proba_negative_dict[query] = {}\n",
    "    proba_negative_dict[query]['attr_list'] = attr_list\n",
    "    proba_list = []\n",
    "    for attr in attr_list:\n",
    "        proba_list.append(0)\n",
    "    proba_negative_dict[query]['attr_freq'] = proba_list\n",
    "\n",
    "\n",
    "query_list = list(attr_dict.keys())\n",
    "with open(file, 'r') as f:\n",
    "    for i, data in enumerate(tqdm(f)):\n",
    "        data = json.loads(data)\n",
    "        img_name = data['img_name']\n",
    "        title = data['title']\n",
    "        key_attr = data['key_attr']\n",
    "        new_key_attr = data['new_key_attr']\n",
    "\n",
    "        for query, attr in key_attr.items():\n",
    "            attr_list = proba_negative_dict[query]['attr_list']\n",
    "            idx = attr_list.index(attr)\n",
    "            proba_negative_dict[query]['attr_freq'][idx] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1986.9346"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "30 + 1452 * 0.9373 + 299 * 0.9565 + 331 * 0.9365"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9408143939393939"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "衣服 1452个 0.9373\n",
    "裤子 299 0.9565 \n",
    "鞋 331 0.9365 \n",
    "包 30 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm \n",
    "import json\n",
    "import itertools\n",
    "import random \n",
    "\n",
    "# 加载新的属性字典\n",
    "attr_file = 'data/equal_processed_data/attr_to_attrvals.json'\n",
    "with open(attr_file, 'r') as f:\n",
    "    attr_dict = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10000it [00:00, 64337.51it/s]\n"
     ]
    }
   ],
   "source": [
    "file = 'data/equal_processed_data/nofeat/test10000.txt'\n",
    "save_file = 'data/equal_processed_data/nofeat/test10000_analysis.txt'\n",
    "\n",
    "rets = []\n",
    "query_list = list(attr_dict.keys())\n",
    "with open(file, 'r') as f:\n",
    "    for i, data in enumerate(tqdm(f)):\n",
    "        data = json.loads(data)\n",
    "        title = data['title']\n",
    "        key_attr = {}\n",
    "        \n",
    "        # 属性提取\n",
    "        for query in query_list:\n",
    "            attr_list = attr_dict[query]\n",
    "            for attr in attr_list:\n",
    "                if attr in title:\n",
    "                    key_attr[query] = attr\n",
    "                        \n",
    "        data['new_key_attr'] = key_attr\n",
    "        rets.append(json.dumps(data, ensure_ascii=False)+'\\n')\n",
    "        \n",
    "with open(save_file, 'w') as f:\n",
    "    f.writelines(rets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10000it [00:00, 83007.69it/s]\n"
     ]
    }
   ],
   "source": [
    "file = 'pred_title_attr_set1_B.txt'\n",
    "\n",
    "preds = []\n",
    "with open(file, 'r') as f:\n",
    "    for i, data in enumerate(tqdm(f)):\n",
    "        data = json.loads(data)\n",
    "        preds.append(data)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10000it [00:00, 176435.87it/s]\n"
     ]
    }
   ],
   "source": [
    "file = 'data/equal_processed_data/nofeat/test10000_analysis.txt'\n",
    "\n",
    "count_dict = {}\n",
    "pos_count_dict = {}\n",
    "count_query = '领型'\n",
    "\n",
    "query_list = list(attr_dict.keys())\n",
    "with open(file, 'r') as f:\n",
    "    for i, data in enumerate(tqdm(f)):\n",
    "        data = json.loads(data)\n",
    "        img_name = data['img_name']\n",
    "        title = data['title']\n",
    "        key_attr = data['key_attr']\n",
    "        new_key_attr = data['new_key_attr']\n",
    "        \n",
    "        match = preds[i]['match']\n",
    "        \n",
    "        if count_query in new_key_attr.keys():\n",
    "            attr = new_key_attr[count_query]\n",
    "            if attr not in count_dict:\n",
    "                count_dict[attr] = 1\n",
    "            else:\n",
    "                count_dict[attr] += 1\n",
    "            if count_query in match:\n",
    "                if match[count_query] == 1:\n",
    "                    if attr not in pos_count_dict:\n",
    "                        pos_count_dict[attr] = 1\n",
    "                    else:\n",
    "                        pos_count_dict[attr] += 1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = [12221, 13685, 11393, 26, 4175, 70, 46, 177, 407, 3525, 908, 12853, 13, 11]\n",
    "N = len(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = [6122,34906,418,747]\n",
    "N = len(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = 0.4\n",
    "new_l = []\n",
    "for i in range(N):\n",
    "    n = l[i]\n",
    "    pos = n * (1-p)\n",
    "    neg = 0\n",
    "    for j in range(N):\n",
    "        if j != i:\n",
    "            neg += l[j] * p / (N-1)\n",
    "    new_l.append(int(pos+neg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[8482, 21915, 5820, 5974]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'圆领': 659, '连帽': 715, '亨利领': 113, 'V领': 284, 'U型领': 134, '高领': 647, '棒球领': 161, '一字领': 125, '西装领': 319, '翻领': 681, '斜领': 111, '双层领': 137, '堆堆领': 134, '围巾领': 122}\n",
      "{'圆领': 576, '连帽': 629, 'V领': 176, '高领': 551, '西装领': 201, '翻领': 590, 'U型领': 7, '棒球领': 41, '堆堆领': 15, '围巾领': 9}\n"
     ]
    }
   ],
   "source": [
    "print(count_dict)\n",
    "print(pos_count_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "45000it [00:00, 179374.76it/s]\n"
     ]
    }
   ],
   "source": [
    "file = 'data/equal_processed_data/nofeat/fine45000.txt'\n",
    "\n",
    "count_dict = {}\n",
    "\n",
    "count_query = '领型'\n",
    "\n",
    "query_list = list(attr_dict.keys())\n",
    "with open(file, 'r') as f:\n",
    "    for i, data in enumerate(tqdm(f)):\n",
    "        data = json.loads(data)\n",
    "        img_name = data['img_name']\n",
    "        title = data['title']\n",
    "        key_attr = data['key_attr']\n",
    "\n",
    "        if count_query in key_attr.keys():\n",
    "            attr = key_attr[count_query]\n",
    "            if attr not in count_dict:\n",
    "                count_dict[attr] = 1\n",
    "            else:\n",
    "                count_dict[attr] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_list = [6122,34906,418,747]\n",
    "p = 0.9296"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6122 430.9888000000001\n",
      "34906 2457.3824000000004\n",
      "418 29.427200000000006\n",
      "747 52.58880000000001\n"
     ]
    }
   ],
   "source": [
    "S = sum(num_list)\n",
    "N = len(num_list)\n",
    "for n in num_list:\n",
    "    n_pos = n*(1-p)\n",
    "    print(n, n_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'高领': 3684,\n",
       " '翻领': 3900,\n",
       " '连帽': 4541,\n",
       " 'V领': 1209,\n",
       " '圆领': 4331,\n",
       " '西装领': 1366,\n",
       " '围巾领': 77,\n",
       " '棒球领': 307,\n",
       " 'U型领': 33,\n",
       " '堆堆领': 161,\n",
       " '一字领': 23,\n",
       " '亨利领': 6,\n",
       " '斜领': 6,\n",
       " '双层领': 12}"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8663729809104258"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "590 / 681"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8567083474146672"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2535 / 2959"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2824.6153846153848\n",
      "2959.2\n",
      "3358.5923076923077\n",
      "1282.5\n",
      "3227.746153846154\n",
      "1380.3230769230768\n",
      "577.176923076923\n",
      "720.4846153846154\n",
      "549.7615384615384\n",
      "629.5153846153846\n",
      "543.5307692307692\n",
      "532.9384615384615\n",
      "532.9384615384615\n",
      "536.676923076923\n"
     ]
    }
   ],
   "source": [
    "# l = [280, 707, 344, 350]\n",
    "# l = [993, 4691]\n",
    "l = list(count_dict.values())\n",
    "p = 0.35 # 负例概率\n",
    "for x in l:\n",
    "    y = x * (1-p) + (sum(l) - x) * p / (len(l) - 1)\n",
    "    print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2394.6\n",
      "2535.0\n",
      "2951.65\n",
      "785.85\n",
      "2815.15\n",
      "887.9\n",
      "50.050000000000004\n",
      "199.55\n",
      "21.45\n",
      "104.65\n",
      "14.950000000000001\n",
      "3.9000000000000004\n",
      "3.9000000000000004\n",
      "7.800000000000001\n"
     ]
    }
   ],
   "source": [
    "for x in l:\n",
    "    y = x * (1-p)\n",
    "    print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7592067988668555"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1876 / 2471"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7708674304418985"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "471 / 611"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10000it [00:00, 166552.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2409\n",
      "0\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "file = 'data/equal_processed_data/nofeat/test10000_analysis.txt'\n",
    "\n",
    "attr_count = 0\n",
    "count = 0\n",
    "pos_count = 0\n",
    "query_list = list(attr_dict.keys())\n",
    "with open(file, 'r') as f:\n",
    "    for i, data in enumerate(tqdm(f)):\n",
    "        data = json.loads(data)\n",
    "        img_name = data['img_name']\n",
    "        title = data['title']\n",
    "        key_attr = data['key_attr']\n",
    "        new_key_attr = data['new_key_attr']\n",
    "        \n",
    "        # if '类别' in key_attr.keys():\n",
    "        #     print(img_name)\n",
    "        #     print(preds[i]['match'])\n",
    "        #     print(key_attr)\n",
    "        #     print(new_key_attr)\n",
    "        #     print()\n",
    "        #     count += 1\n",
    "        #     if preds[i]['match']['图文'] == 1:\n",
    "        #         pos_count += 1\n",
    "        \n",
    "        # if any(q in key_attr.keys() for q in ['领型', '袖长', '衣长', '版型', '裙长', '穿着方式']):\n",
    "        #     print(img_name)\n",
    "        #     print(preds[i]['match'])\n",
    "        #     print(key_attr)\n",
    "        #     print(new_key_attr)\n",
    "        #     print()\n",
    "        #     count += 1\n",
    "        #     if preds[i]['match']['图文'] == 1:\n",
    "        #         pos_count += 1\n",
    "        \n",
    "        # if any(q in key_attr.keys() for q in ['裤型', '裤长', '裤门襟']):\n",
    "        #     print(img_name)\n",
    "        #     print(preds[i]['match'])\n",
    "        #     print(key_attr)\n",
    "        #     # print(new_key_attr)\n",
    "        #     print()\n",
    "        #     count += 1\n",
    "        #     if preds[i]['match']['图文'] == 1:\n",
    "        #         pos_count += 1\n",
    "        \n",
    "        # if any(q in key_attr.keys() for q in ['闭合方式', '鞋帮高度']):\n",
    "        #     print(img_name)\n",
    "        #     print(preds[i]['match'])\n",
    "        #     print(key_attr)\n",
    "        #     # print(new_key_attr)\n",
    "        #     print()\n",
    "        #     count += 1\n",
    "        #     if preds[i]['match']['图文'] == 1:\n",
    "        #         pos_count += 1\n",
    "                \n",
    "        # if key_attr == new_key_attr:\n",
    "        #     if len(key_attr) == 1:\n",
    "        #         count += 1\n",
    "        #         if preds[i]['match']['图文'] == 1:\n",
    "        #             pos_count += 1\n",
    "        #         print(img_name)\n",
    "        #         print(preds[i]['match'])\n",
    "        #         print(key_attr)\n",
    "        #         print(new_key_attr)\n",
    "        #         print()\n",
    "        \n",
    "        if '类别' not in key_attr.keys(): # 去除包\n",
    "            if preds[i]['match']['图文'] == 0: # 负例\n",
    "                if key_attr == new_key_attr: # 去除隐藏属性的样本\n",
    "                    if len(key_attr) >= 2: # 属性不少于两个\n",
    "                        flag_1 = 0\n",
    "                        flag_0 = 0\n",
    "                        for key, value in preds[i]['match'].items():\n",
    "                            if key != '图文':\n",
    "                                if value == 1:\n",
    "                                    flag_1 = 1\n",
    "                                else:\n",
    "                                    flag_0 = 1\n",
    "                        if flag_0 and flag_1:\n",
    "                            count += 1\n",
    "                    \n",
    "                    \n",
    "            \n",
    "            \n",
    "        # if key_attr != new_key_attr:\n",
    "        #     # count += 1\n",
    "        #     # if preds[i]['match']['图文'] == 1:\n",
    "        #     #     pos_count += 1\n",
    "        #     for key, value in preds[i]['match'].items():\n",
    "        #         if key != '图文':\n",
    "        #             attr_count += 1\n",
    "        #             if value == 1:\n",
    "        #                 pos_count += 1\n",
    "        #     print(img_name)\n",
    "        #     print(preds[i]['match'])\n",
    "        #     print(title)\n",
    "        #     print(key_attr)\n",
    "        #     print(new_key_attr)\n",
    "        #     print()\n",
    "        \n",
    "        # if preds[i]['match']['图文'] == 0:\n",
    "        #     if '类别' not in key_attr.keys():\n",
    "        #         if key_attr == new_key_attr:\n",
    "        #             count += 1\n",
    "        #             for key, value in preds[i]['match'].items():\n",
    "        #                 if key != '图文':\n",
    "        #                     attr_count += 1\n",
    "        #                     if value == 1:\n",
    "        #                         pos_count += 1\n",
    "                \n",
    "        \n",
    "        # if preds[i]['match']['图文'] == 1:\n",
    "        #     count += 1\n",
    "            \n",
    "        if i > 10000:\n",
    "            break\n",
    "print(count)\n",
    "print(pos_count)\n",
    "print(attr_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6776"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "10000 - 363 - (451 + 2000 + 410)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.14757969303423848"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1000/ 6776"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10000it [00:00, 326661.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "file = 'pred_title_attr_set1_B.txt'\n",
    "\n",
    "count = 0\n",
    "with open(file, 'r') as f:\n",
    "    for i, data in enumerate(tqdm(f)):\n",
    "        data = json.loads(data)\n",
    "        if data['match']['图文'] == 1:\n",
    "            count += 1\n",
    "            \n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.306"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1224 / 4000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5000it [00:00, 154794.21it/s]\n"
     ]
    }
   ],
   "source": [
    "file = 'data/equal_processed_data/nofeat/fine5000.txt'\n",
    "save_file = 'data/equal_processed_data/nofeat/mytest5000.txt'\n",
    "\n",
    "rets = []\n",
    "with open(file, 'r') as f:\n",
    "    for i, data in enumerate(tqdm(f)):\n",
    "        data = json.loads(data)\n",
    "        img_name = data['img_name']\n",
    "        title = data['title']\n",
    "        key_attr = data['key_attr']\n",
    "        match = data['match']\n",
    "        \n",
    "        if '类别' in key_attr.keys():\n",
    "            if random.random() < 1/3:\n",
    "                match['类别'] = 0\n",
    "                match['图文'] = 0\n",
    "                \n",
    "        else:\n",
    "            flag = 1\n",
    "            for key, value in key_attr.items():\n",
    "                if random.random() < 0.62:\n",
    "                    match[key] = 0\n",
    "                    flag = 0\n",
    "                if flag == 0:\n",
    "                    match['图文'] = 0\n",
    "                \n",
    "        rets.append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "893\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "pos_count = 0\n",
    "\n",
    "\n",
    "for data in rets:\n",
    "    img_name = data['img_name']\n",
    "    title = data['title']\n",
    "    key_attr = data['key_attr']\n",
    "    match = data['match']\n",
    "    if match['图文'] == 1:\n",
    "        pos_count += 1\n",
    "        \n",
    "print(pos_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5000it [00:00, 54004.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import random \n",
    "random.seed(0)\n",
    "\n",
    "attr_dict_file = 'data/equal_processed_data/attr_to_attrvals.json'\n",
    "with open(attr_dict_file, 'r') as f:\n",
    "    attr_dict = json.load(f)\n",
    "\n",
    "def get_negative_dict(attr_dict):\n",
    "        negative_dict = {}\n",
    "        for query, attr_list in attr_dict.items():\n",
    "            negative_dict[query] = {}\n",
    "            for attr in attr_list:\n",
    "                l = attr_list.copy()\n",
    "                l.remove(attr)\n",
    "                negative_dict[query][attr] = l\n",
    "        return negative_dict\n",
    "negative_dict = get_negative_dict(attr_dict)\n",
    "\n",
    "\n",
    "file = 'data/equal_processed_data/nofeat/fine5000.txt'\n",
    "save_file = 'data/equal_processed_data/nofeat/mytest5000_nohide.txt'\n",
    "\n",
    "rets = []\n",
    "with open(file, 'r') as f:\n",
    "    for i, data in enumerate(tqdm(f)):\n",
    "        data = json.loads(data)\n",
    "        img_name = data['img_name']\n",
    "        title = data['title']\n",
    "        key_attr = data['key_attr']\n",
    "        match = data['match']\n",
    "        \n",
    "        # 包 永远只有一个属性，1/3的概率为负\n",
    "        if '类别' in key_attr.keys():\n",
    "            if random.random() < 1/3:\n",
    "                query = '类别'\n",
    "                attr = key_attr[query]\n",
    "                new_attr = random.sample(negative_dict[query][attr], 1)[0]\n",
    "                key_attr[query] = new_attr\n",
    "                title = title.replace(attr, new_attr)\n",
    "                match['类别'] = 0\n",
    "                match['图文'] = 0\n",
    "        # 其他样本，29.2%的概率为正样本\n",
    "        else:\n",
    "            if random.random() < (1 - 0.292): # 以此概率为负\n",
    "                match['图文'] = 0\n",
    "                # 先随机选取一个属性为负\n",
    "                query = random.sample(list(key_attr.keys()), 1)[0]\n",
    "                attr = key_attr[query]\n",
    "                new_attr = random.sample(negative_dict[query][attr], 1)[0]\n",
    "                key_attr[query] = new_attr\n",
    "                title = title.replace(attr, new_attr)\n",
    "                match[query] = 0\n",
    "                # 其余属性以一定概率变负\n",
    "                for query, attr in key_attr.items():\n",
    "                    if match[query] != 0:\n",
    "                        if random.random() < 0.434:\n",
    "                            new_attr = random.sample(negative_dict[query][attr], 1)[0]\n",
    "                            key_attr[query] = new_attr\n",
    "                            title = title.replace(attr, new_attr)\n",
    "                            match[query] = 0\n",
    "                # 隐藏属性\n",
    "                # if len(key_attr) >= 2: # 属性不少于两个\n",
    "                #     flag_0 = 0\n",
    "                #     flag_1 = 0\n",
    "                #     for query, value in match.items(): # 有正有负\n",
    "                #         if query != '图文':\n",
    "                #             if value == 1:\n",
    "                #                 flag_1 = 1\n",
    "                #             else:\n",
    "                #                 flag_0 = 1\n",
    "                #     if flag_0 and flag_1:\n",
    "                #         if random.random() < 0.3: # 一定概率进行隐藏\n",
    "                #             match_copy = match.copy()\n",
    "                #             for query, value in match_copy.items():\n",
    "                #                 if query != '图文':\n",
    "                #                     if value == 0:\n",
    "                #                         del match[query]\n",
    "                #                         del key_attr[query]\n",
    "            \n",
    "        data['title'] = title \n",
    "        data['key_attr'] = key_attr \n",
    "        data['match'] = match \n",
    "        rets.append(json.dumps(data, ensure_ascii=False)+'\\n')\n",
    "\n",
    "print(len(rets))\n",
    "with open(save_file, 'w') as f:\n",
    "    f.writelines(rets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5000it [00:01, 3674.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10932\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "file = 'data/equal_processed_data/mytest5000_hide.txt'\n",
    "\n",
    "attr_count = 0\n",
    "count = 0\n",
    "pos_count = 0\n",
    "query_list = list(attr_dict.keys())\n",
    "with open(file, 'r') as f:\n",
    "    for i, data in enumerate(tqdm(f)):\n",
    "        data = json.loads(data)\n",
    "        img_name = data['img_name']\n",
    "        title = data['title']\n",
    "        key_attr = data['key_attr']\n",
    "        match = data['match']\n",
    "        for query, attr in key_attr.items():\n",
    "            attr_count += 1\n",
    "            \n",
    "print(attr_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5000it [00:01, 3687.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1899\n",
      "0\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "file = 'data/equal_processed_data/mytest5000_hide.txt'\n",
    "\n",
    "attr_count = 0\n",
    "count = 0\n",
    "pos_count = 0\n",
    "query_list = list(attr_dict.keys())\n",
    "with open(file, 'r') as f:\n",
    "    for i, data in enumerate(tqdm(f)):\n",
    "        data = json.loads(data)\n",
    "        img_name = data['img_name']\n",
    "        title = data['title']\n",
    "        key_attr = data['key_attr']\n",
    "        match = data['match']\n",
    "        \n",
    "        # if '类别' in key_attr.keys():\n",
    "        #     print(img_name)\n",
    "        #     print(preds[i]['match'])\n",
    "        #     print(key_attr)\n",
    "        #     print(new_key_attr)\n",
    "        #     print()\n",
    "        #     count += 1\n",
    "        #     if preds[i]['match']['图文'] == 1:\n",
    "        #         pos_count += 1\n",
    "        \n",
    "        # if any(q in key_attr.keys() for q in ['领型', '袖长', '衣长', '版型', '裙长', '穿着方式']):\n",
    "        #     print(img_name)\n",
    "        #     print(preds[i]['match'])\n",
    "        #     print(key_attr)\n",
    "        #     print(new_key_attr)\n",
    "        #     print()\n",
    "        #     count += 1\n",
    "        #     if preds[i]['match']['图文'] == 1:\n",
    "        #         pos_count += 1\n",
    "        \n",
    "        # if any(q in key_attr.keys() for q in ['裤型', '裤长', '裤门襟']):\n",
    "        #     print(img_name)\n",
    "        #     print(preds[i]['match'])\n",
    "        #     print(key_attr)\n",
    "        #     # print(new_key_attr)\n",
    "        #     print()\n",
    "        #     count += 1\n",
    "        #     if preds[i]['match']['图文'] == 1:\n",
    "        #         pos_count += 1\n",
    "        \n",
    "        # if any(q in key_attr.keys() for q in ['闭合方式', '鞋帮高度']):\n",
    "        #     print(img_name)\n",
    "        #     print(preds[i]['match'])\n",
    "        #     print(key_attr)\n",
    "        #     # print(new_key_attr)\n",
    "        #     print()\n",
    "        #     count += 1\n",
    "        #     if preds[i]['match']['图文'] == 1:\n",
    "        #         pos_count += 1\n",
    "                \n",
    "        # if key_attr == new_key_attr:\n",
    "        #     if len(key_attr) == 1:\n",
    "        #         count += 1\n",
    "        #         if preds[i]['match']['图文'] == 1:\n",
    "        #             pos_count += 1\n",
    "        #         print(img_name)\n",
    "        #         print(preds[i]['match'])\n",
    "        #         print(key_attr)\n",
    "        #         print(new_key_attr)\n",
    "        #         print()\n",
    "        \n",
    "        if '类别' not in key_attr.keys(): # 去除包\n",
    "            if match['图文'] == 0: # 负例\n",
    "                if len(key_attr) >= 2: # 属性不少于两个\n",
    "                    flag_1 = 0\n",
    "                    flag_0 = 0\n",
    "                    for key, value in match.items():\n",
    "                        if key != '图文':\n",
    "                            if value == 1:\n",
    "                                flag_1 = 1\n",
    "                            else:\n",
    "                                flag_0 = 1\n",
    "                    if flag_0 and flag_1:\n",
    "                        count += 1\n",
    "                    \n",
    "                    \n",
    "            \n",
    "            \n",
    "        # if key_attr != new_key_attr:\n",
    "        #     # count += 1\n",
    "        #     # if preds[i]['match']['图文'] == 1:\n",
    "        #     #     pos_count += 1\n",
    "        #     for key, value in preds[i]['match'].items():\n",
    "        #         if key != '图文':\n",
    "        #             attr_count += 1\n",
    "        #             if value == 1:\n",
    "        #                 pos_count += 1\n",
    "        #     print(img_name)\n",
    "        #     print(preds[i]['match'])\n",
    "        #     print(title)\n",
    "        #     print(key_attr)\n",
    "        #     print(new_key_attr)\n",
    "        #     print()\n",
    "        \n",
    "        # if preds[i]['match']['图文'] == 0:\n",
    "        #     if '类别' not in key_attr.keys():\n",
    "        #         if key_attr == new_key_attr:\n",
    "        #             count += 1\n",
    "        #             for key, value in preds[i]['match'].items():\n",
    "        #                 if key != '图文':\n",
    "        #                     attr_count += 1\n",
    "        #                     if value == 1:\n",
    "        #                         pos_count += 1\n",
    "                \n",
    "        \n",
    "        # if preds[i]['match']['图文'] == 1:\n",
    "        #     count += 1\n",
    "            \n",
    "        if i > 10000:\n",
    "            break\n",
    "print(count)\n",
    "print(pos_count)\n",
    "print(attr_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1113.02"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1919 * 2 * 0.29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5000it [00:00, 103325.80it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.7085820400583632, 10684, 15078)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "from tqdm import tqdm\n",
    "result_file = 'output/fusion/predict/attr_0.txt'\n",
    "\n",
    "c = 0\n",
    "a = 0\n",
    "with open(result_file, \"r\") as f:\n",
    "    for line in tqdm(f):\n",
    "        item = json.loads(line)\n",
    "        for key, value in item[\"pred\"].items():\n",
    "            if key!=[\"图文\"]:\n",
    "                if int(value>0.5) == int(item[\"match\"][key]>0.5):\n",
    "                    c+=1\n",
    "            a+=1\n",
    "c/a, c, a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5000it [00:00, 112435.17it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.9360984322286168, 9434, 10078)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "from tqdm import tqdm\n",
    "result_file = 'output/fusion/predict/attr_0.txt'\n",
    "\n",
    "c = 0\n",
    "a = 0\n",
    "with open(result_file, \"r\") as f:\n",
    "    for line in tqdm(f):\n",
    "        item = json.loads(line)\n",
    "        # print(item)\n",
    "        for key, value in item[\"pred\"].items():\n",
    "            # print(key, value)\n",
    "            if key!=\"图文\":\n",
    "                if int(value>0.5) == int(item[\"match\"][key]>0.5):\n",
    "                    c+=1\n",
    "                a+=1\n",
    "            # else:\n",
    "            #     print(\"1\")\n",
    "        # break\n",
    "c/a, c, a"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a7cf278074aea429de13d88b394304564a7b018f3e18e13089daf4fad90abe0d"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
